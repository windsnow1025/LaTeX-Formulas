\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{amssymb}

\title{Probability}
\date{}

\begin{document}
\maketitle

\tableofcontents

\newpage
\section{Combinatorics}

\subsection{Combinations}
\[
    \binom{n}{k} = \frac{n!}{k!(n-k)!}
\]

\subsection{Permutations}
\[
    P(n, k) = \frac{n!}{(n-k)!}
\]

\subsection{Binomial Theorem}
\[
    (a+b)^n = \sum_{k=0}^{n} \binom{n}{k} a^k b^{n-k}
\]

\newpage
\section{Events}

\subsection{Mutually Exclusive Events}
\[
    \Pr(A \cap B) = 0
\]

\subsection{Complementary Events}
\[
    \Pr(A \cap B) = 0
\]
\[
    \Pr(A) + \Pr(B) = 1
\]

\subsection{Independent Events}
\[
    \Pr(A \cap B) = \Pr(A)\Pr(B)
\]
\[
    \Pr(A|B) = \Pr(A)
\]
\[
    \Pr(B|A) = \Pr(B)
\]

\paragraph{Conditional Independence: }\( A \perp\!\!\!\perp B \mid C \)
\[
    \Pr(A \cap B \mid C) = \Pr(A \mid C) \Pr(B \mid C)
\]
\[
    \Pr(A \mid B, C) = \Pr(A \mid C)
\]
\[
    \Pr(B \mid A, C) = \Pr(B \mid C)
\]


\newpage
\section{Laws of Probability}

\subsection{Complement Rule}
\[
    \Pr(\bar{A}) = 1 - \Pr(A)
\]

\subsection{Addition Law}
\[
    \Pr(A \cup B) = \Pr(A) + \Pr(B) - \Pr(A \cap B)
\]

\subsection{Subtraction Law}
\[
    \Pr(A \setminus B) = \Pr(A \cap \bar{B}) = \Pr(A) - \Pr(A \cap B)
\]

\subsection{Conditional Probability}
\[
    \Pr(A|B) = \frac{\Pr(A \cap B)}{\Pr(B)}
\]

\subsection{Law of Total Probability}
\[
    \Pr(A) = \sum_{i=1}^{k} \Pr(B_i \cap A) = \sum_{i=1}^{k} \Pr(B_i)\Pr(A|B_i)
\]

\subsection{Bayes Theorem}
\[
    \Pr(B_i|A) = \frac{\Pr(B_i)\Pr(A|B_i)}{\Pr(A)}
\]

\newpage
\section{Numerical Characteristics}

\subsection{Expected Value (Mean)}
\[
    \mu = \mathbb{E}_X[X] = \int_{-\infty}^{+\infty} x f_X(x) dx
\]
\[
    \mu = \mathbb{E}_X[X] = \sum_{x} x \Pr(X=x)
\]
\textbf{Properties:}
\begin{itemize}
    \item \( \mathbb{E}[c] = c \)
    \item \( \mathbb{E}[aX+bY] = a\mathbb{E}[X] + b\mathbb{E}[Y] \)
    \item \( \mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y] + \text{Cov}(X,Y) \)
    \item \( \mathbb{E}[(X-a)^2] = (\mathbb{E}[X]-a)^2 + \text{Var}(X) \)
    \item \( X, Y \) i.i.d. with \( \sigma^2 \): \( \mathbb{E}[(X-Y)^2] = 2\sigma^2 \)
\end{itemize}

\subsubsection{Expected Value of a function}
\[
    \mathbb{E}_X[g(X)] = \int_{-\infty}^{+\infty} g(x)f_X(x)dx
\]

\subsubsection{Law of Total Expectation}
\[
    \mathbb{E}_{X,Y}[X] = \mathbb{E}_Y[\mathbb{E}_{X|Y}[X|Y]]
\]

\subsection{Variance}
\[
    \sigma^2 = Var(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - \mathbb{E}[X]^2
\]
\textbf{Properties:}
\begin{itemize}
    \item \( Var(c) = 0 \)
    \item \( Var(cX) = c^2 Var(X) \)
    \item \( Var(X+c) = Var(X) \)
    \item \( Var(X \pm Y) = Var(X) + Var(Y) \pm 2\text{Cov}(X,Y) \)
\end{itemize}

\subsection{Standard Deviation}
\[
    \sigma = \sqrt{\mathbb{E}[X-\mu]^2}
\]

\subsection{Covariance}
\[
    \text{Cov}(X,Y) = \mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
\]
\textbf{Properties:}
\begin{itemize}
    \item \( \text{Cov}(X, X) = \text{Var}(X) \)
    \item \( \text{Cov}(X,Y) = \text{Cov}(Y,X) \)
    \item \( \text{Cov}(aX, bY) = ab\,\text{Cov}(X,Y) \)
    \item \( \text{Cov}(c,X) = 0 \)
    \item \( \text{Cov}(X_1+X_2, Y) = \text{Cov}(X_1,Y) + \text{Cov}(X_2,Y) \)
    \item If X and Y are mutually independent, \( \text{Cov}(X,Y) = 0 \)
\end{itemize}

\subsection{Covariance Matrix}
\[
    \Sigma = E[(X - E[X])(X - E[X])^T]
\]
\textbf{Properties:}
\begin{itemize}
    \item \(\text{Cov}(AX + b) = A \, \text{Cov}(X) \, A^T\)
\end{itemize}

\subsection{Correlation Coefficient}
\[
    \rho_{X,Y} = \frac{\text{Cov}(X,Y)}{\sigma_X\sigma_Y}
\]
\textbf{Properties:}
\begin{itemize}
    \item \( |\rho_{X,Y}| \le 1 \)
    \item \( |\rho_{X,Y}| = 1 \Leftrightarrow (\exists a)(\exists b)(\Pr(Y=aX+b)=1), (a>0, \rho_{X,Y}=1; a<0, \rho_{X,Y}=-1) \)
    \item \( \rho_{aX + b, cY + d} = \rho_{X,Y} \)
\end{itemize}

\subsection{Markov's Inequality}
If \( X \geq 0 \), for any \( a > 0 \):
\[
    \Pr(X \geq a) \leq \frac{\mathbb{E}[X]}{a}
\]

\subsection{Chebyshev's Inequality}
For any \( \varepsilon > 0 \):
\[
    \Pr(|X-\mathbb{E}[X]| \ge \varepsilon) \le \frac{Var(X)}{\varepsilon^2}
\]
\[
    \Pr(|X-\mathbb{E}[X]| < \varepsilon) \ge 1 - \frac{Var(X)}{\varepsilon^2}
\]

\newpage
\section{One-dimensional Random Variables}

\subsection{Probability Mass Function}
For a discrete random variable, the Probability Mass Function \( p(x) \) is defined as:
\[
    \Pr(X=x) = p(x)
\]

\subsection{Probability Density Function}
For a continuous random variable, the Probability Density Function \( f(x) \) is defined as:
\[
    \Pr(a \le x \le b) = \int_{a}^{b} f_X(x)dx
\]

\subsection{Cumulative Distribution Function}
\[
    F(x) = \Pr(X \le x) = \int_{-\infty}^{x} f_X(t)dt
\]

\subsubsection{Cumulative Distribution Function of a function}
\[
    Y=g(X)
\]
\[
    F(y) = \Pr(Y \le y) = \int_{g(x) \le y} f_X(x)dx
\]

\subsection{Discrete Probability Distributions}

\subsubsection{Discrete Uniform Distribution}
\[
    X \sim U\{a, a+1, \dots, b\}
\]
\[
    p(x) =
    \begin{cases}
        \frac{1}{n}, & x \in \{a, a+1, \dots, b\} \\
        0, & \text{otherwise}
    \end{cases}
\]
where $n = b - a + 1$.

\[
    \mu = \frac{a + b}{2}
\]
\[
    \sigma^2 = \frac{n^2 - 1}{12}
\]

\subsubsection{Bernoulli Distribution}
Probability of success in a single trial with only two outcomes.
\[ X \sim B(1,p) \]
\[ p(x) = p^x (1-p)^{1-x}, \quad x=0,1 \]
\[ \mu = p \]
\[ \sigma^2 = p(1-p) \]

\subsubsection{Binomial Distribution}
The number of successes in \( n \) independent Bernoulli trails.
\[ X \sim B(n,p) \]
\[ p(x) = \binom{n}{x} p^x (1-p)^{n-x}, \quad x=0,1,\dots,n \]
\[ \mu = np \]
\[ \sigma^2 = np(1-p) \]

\subsubsection{Geometric Distribution}
The number of trials needed to get the first success in a sequence of independent Bernoulli trials.
\[ X \sim G(p) \]
\[ p(x) = (1-p)^{x-1}p, \quad x=1,2,\dots \]
\[ F(x) = 1 - (1-p)^x \]
\[ \mu = \frac{1}{p} \]
\[ \sigma^2 = \frac{1-p}{p^2} \]

\subsubsection{Poisson Distribution}
The number of events occurring in an interval of time, given an average rate \( \lambda \) of occurrence.
\[ X \sim \Pr(\lambda) \]
\[ p(x) = \frac{\lambda^x e^{-\lambda}}{x!}, \quad \lambda>0, x=0,1,\dots \]
\[ \mu = \sigma^2 = \lambda \]
\textbf{Derivation (Poisson Limit Theorem):}
\[
    B(n, p) \xrightarrow{n \to \infty,\, p \to 0} \Pr(\lambda = np)
\]

\subsection{Continuous Probability Distributions}

\subsubsection{Uniform Distribution}
\[
    X \sim U(a, b)
\]
\[
    f(x) =
    \begin{cases}
        \frac{1}{b-a}, & a \leq x \leq b \\
        0, & \text{otherwise}
    \end{cases}
\]
\[
    F(x) =
    \begin{cases}
        0, & x < a \\
        \frac{x-a}{b-a}, & a \leq x \leq b \\
        1, & x > b
    \end{cases}
\]
\[
    \mu = \frac{a+b}{2}
\]
\[
    \sigma^2 = \frac{(b-a)^2}{12}
\]

\subsubsection{Exponential Distribution}
The waiting time for the next event in a Poisson process with an average rate \( \lambda \).
\[ X \sim E(\lambda) \]
\[
    f(x) = \lambda e^{-\lambda x}, \quad x \geq 0
\]
\[
    F(x) = 1 - e^{-\lambda x}, \quad x \geq 0
\]
\[
    \mu = \frac{1}{\lambda}
\]
\[
    \sigma^2 = \frac{1}{\lambda^2}
\]
Derivation from Poisson:
\[
    \Pr(X>x) = f_{\text{Poisson}(\lambda x)}(0)
\]

\subsubsection{Gamma Distribution}
The waiting time for the \( r^{th} \) event in a Poisson process with an average rate \( \lambda \).
\[ X \sim \mathrm{Gamma}(r, \lambda) \]
\[
    f(x) = \frac{\lambda^r x^{r-1} e^{-\lambda x}}{(r-1)!}, \quad x \geq 0
\]
\[
    \mu = \frac{r}{\lambda}
\]
\[
    \sigma^2 = \frac{r}{\lambda^2}
\]
Derivation from Poisson:
\[
    \Pr(X > x) = \sum_{k=0}^{r-1} f_{\text{Poisson}(\lambda x)}(k)
\]

\subsubsection{Normal Distribution}
\[ X \sim \mathcal{N}(\mu, \sigma^2) \]
\[
    f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right)
\]
\textbf{Derivation (De Moivreâ€“Laplace theorem):}
\[
    B(n, p) \xrightarrow{n \to \infty} \mathcal{N}(\mu = np, \sigma^2 = np(1-p))
\]

\subsubsection{Standard Normal Distribution}
\[ Z = \frac{X-\mu}{\sigma} \sim \mathcal{N}(0,1) \]
\[
    \phi(z) = \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{z^2}{2} \right)
\]
\[
    \Phi(x) = \int_{-\infty}^{x} \phi(t)dt
\]

\newpage
\section{Two-dimensional Continuous Random Variable}

\subsection{Probability Distribution Function}
For two continuous random variables, the Probability Density Function \( f(x,y) \) is defined as:
\[
    \Pr((X,Y) \in A) = \iint_{A} f(x,y) dx dy
\]

\subsection{Cumulative Distribution Function}
\[
    F(x,y) = \int_{-\infty}^{y} \int_{-\infty}^{x} f(u,v) du dv
\]

\subsection{Marginal Probability Density Function}
\[
    f_X(x) = \int_{-\infty}^{\infty} f(x,y)dy
\]
\[
    f_Y(y) = \int_{-\infty}^{\infty} f(x,y)dx
\]

\subsection{Conditional Probability Density Function}
\[
    f_{X|Y}(x|y) = \frac{f(x,y)}{f_Y(y)}
\]
\[
    f_{Y|X}(y|x) = \frac{f(x,y)}{f_X(x)}
\]

\subsection{Convolution of Probability Distributions}
\[
    Z = X + Y
\]
\[
    f_Z(z) = \int_{-\infty}^{+\infty} f(x,z-x)dx
\]
\[
    f_Z(z) = \int_{-\infty}^{+\infty} f(z-y,y)dy
\]

\subsection{Distribution of a Function of Two Random Variables}
\[
    z=g(x,y)
\]
\[
    y=h(x,z)
\]
\[
    f_Z(z) = \int_{-\infty}^{+\infty} f(x,h(x,z)) \left| \frac{\partial h}{\partial z} \right| dx
\]

\subsection{Bivariate Normal Distribution}
\[ (X,Y) \sim \mathcal{N}(\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, \rho) \]
\[
    f(x_1, x_2) = \frac{1}{2\pi\sqrt{\det(\Sigma)}} \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{x} - \boldsymbol{\mu})\right)
\]

\textbf{Properties:}
\begin{itemize}
    \item \( X \sim \mathcal{N}(\mu_1, \sigma_1^2), Y \sim \mathcal{N}(\mu_2, \sigma_2^2) \)
    \item X and Y are mutually independent iff, \( \rho = 0 \)
\end{itemize}

\subsection{Standard Bivariate Normal Distribution}
\[
    f(x_1, x_2) = \frac{1}{2\pi} \exp\left(-\frac{x_1^2 + x_2^2}{2}\right)
\]

\newpage
\section{Multivariate Normal Distribution}

\subsection{Mahalanobis Distance}
\[
    \Delta^2 = (\mathbf{x} - \boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{x} - \boldsymbol{\mu})
\]

\paragraph{Apply Spectral Decomposition:}
\[
    \Sigma^{-1} = (Q \Lambda Q^\top)^{-1} = Q \Lambda^{-1} Q^\top
\]
\[
    \Delta^2 = (\mathbf{x} - \boldsymbol{\mu})^\top (Q \Lambda^{-1} Q^\top) (\mathbf{x} - \boldsymbol{\mu})
\]
Define \( \mathbf{y} = Q^\top (\mathbf{x} - \boldsymbol{\mu}) \):
\[
    \Delta^2 = \mathbf{y}^\top \Lambda^{-1} \mathbf{y} = \sum_{i=1}^{d} \frac{y_i^2}{\lambda_i}
\]

\subsection{Multivariate Normal Distribution}
\[ \mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma) \]
\[
    f(\mathbf{x}; \boldsymbol{\mu}, \Sigma) = \frac{1}{\sqrt{(2\pi)^d\det(\Sigma)}} \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{x} - \boldsymbol{\mu})\right)
\]


\newpage
\section{Information Theory}

\subsection{Quantities of Information}
\[
    I(m) = -\log(p(m))
\]

\subsection{Entropy}
\[
    H(P) = \mathbb{E}[I(X)] = \sum_{i=1}^{n} p_i \cdot I(p_i) = -\sum_{i=1}^{n} p_i \log_2 p_i
\]

\subsection{KL Divergence}
\[
    D_{KL}(P||Q) = \sum_{i=1}^{n} p_i (I(q_i) - I(p_i)) = (-\sum_{i=1}^{n} p_i \log q_i) - (-\sum_{i=1}^{n} p_i \log p_i)
\]

\subsection{Cross Entropy}
\[
    H(P, Q) = -\sum_{i=1}^{n} p_i \log q_i
\]

\end{document}
