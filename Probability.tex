\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{amssymb}

\title{Probability}
\date{}

\begin{document}
\maketitle

\tableofcontents

\newpage
\section{Combinatorics}

\subsection{Combinations}
\[
    \binom{n}{k} = \frac{n!}{k!(n-k)!}
\]

\subsection{Permutations}
\[
    P(n, k) = \frac{n!}{(n-k)!}
\]

\subsection{Binomial Theorem}
\[
    (a+b)^n = \sum_{k=0}^{n} \binom{n}{k} a^k b^{n-k}
\]

\newpage
\section{Events}

\subsection{Mutually Exclusive Events}
\[
    \mathbb{P}(A \cap B) = 0
\]

\subsection{Complementary Events}
\[
    \mathbb{P}(A \cap B) = 0
\]
\[
    \mathbb{P}(A) + \mathbb{P}(B) = 1
\]

\subsection{Independent Events}
\[
    \mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)
\]
\[
    \mathbb{P}(A|B) = \mathbb{P}(A)
\]
\[
    \mathbb{P}(B|A) = \mathbb{P}(B)
\]

\subsubsection{Conditional Independence}
\[
    A \perp\!\!\!\perp B \mid C
\]
\[
    \mathbb{P}(A \cap B \mid C) = \mathbb{P}(A \mid C) \mathbb{P}(B \mid C)
\]
\[
    \mathbb{P}(A \mid B, C) = \mathbb{P}(A \mid C)
\]
\[
    \mathbb{P}(B \mid A, C) = \mathbb{P}(B \mid C)
\]

\subsection{Boole's Inequality (Union Bound)}
\[
    \mathbb{P}\left(\bigcup_{i=1}^{n} A_i\right) \leq \sum_{i=1}^{n} \mathbb{P}(A_i)
\]

\newpage
\section{Laws of Probability}

\subsection{Complement Rule}
\[
    \mathbb{P}(\bar{A}) = 1 - \mathbb{P}(A)
\]

\subsection{Addition Law}
\[
    \mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)
\]

\subsection{Subtraction Law}
\[
    \mathbb{P}(A \setminus B) = \mathbb{P}(A \cap \bar{B}) = \mathbb{P}(A) - \mathbb{P}(A \cap B)
\]

\subsection{Conditional Probability}
\[
    \mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
\]

\subsection{Law of Total Probability}
\[
    \mathbb{P}(A) = \sum_{i=1}^{k} \mathbb{P}(B_i \cap A) = \sum_{i=1}^{k} \mathbb{P}(B_i)\mathbb{P}(A|B_i)
\]

\subsection{Bayes Theorem}
\[
    \mathbb{P}(B_i|A) = \frac{\mathbb{P}(B_i)\mathbb{P}(A|B_i)}{\mathbb{P}(A)}
\]

\newpage
\section{Numerical Characteristics}

\subsection{Expected Value (Mean)}
\[
    \mu = \mathbb{E}_X[X] = \int_{-\infty}^{+\infty} x f_X(x) dx
\]
\[
    \mu = \mathbb{E}_X[X] = \sum_{x} x \mathbb{P}(X=x)
\]
\textbf{Properties:}
\begin{itemize}
    \item $ \mathbb{E}[c] = c $
    \item $ \mathbb{E}[aX+bY] = a\mathbb{E}[X] + b\mathbb{E}[Y] $
    \item $ \mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y] + \operatorname{cov}(X,Y) $
    \item $ \mathbb{E}[(X-a)^2] = (\mathbb{E}[X]-a)^2 + \text{Var}(X) $
    \item $ X, Y $ i.i.d. with $ \sigma^2 $: $ \mathbb{E}[(X-Y)^2] = 2\sigma^2 $
\end{itemize}

\subsubsection{Expected Value of a function}
\[
    \mathbb{E}_X[g(X)] = \int_{-\infty}^{+\infty} g(x)f_X(x)dx
\]

\subsubsection{Law of Total Expectation}
\[
    \mathbb{E}_{X,Y}[X] = \mathbb{E}_Y[\mathbb{E}_{X|Y}[X|Y]]
\]

\subsection{Variance}
\[
    \sigma^2 = Var(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - \mathbb{E}[X]^2
\]
\textbf{Properties:}
\begin{itemize}
    \item $ Var(c) = 0 $
    \item $ Var(cX) = c^2 Var(X) $
    \item $ Var(X+c) = Var(X) $
    \item $ Var(X \pm Y) = Var(X) + Var(Y) \pm 2\operatorname{cov}(X,Y) $
\end{itemize}

\subsection{Standard Deviation}
\[
    \sigma = \sqrt{\mathbb{E}[X-\mu]^2}
\]

\subsection{Covariance}
\[
    \operatorname{cov}(X,Y) = \mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
\]
\textbf{Properties:}
\begin{itemize}
    \item $ \operatorname{cov}(X, X) = \text{Var}(X) $
    \item $ \operatorname{cov}(X,Y) = \operatorname{cov}(Y,X) $
    \item $ \operatorname{cov}(aX, bY) = ab\,\operatorname{cov}(X,Y) $
    \item $ \operatorname{cov}(c,X) = 0 $
    \item $ \operatorname{cov}(X_1+X_2, Y) = \operatorname{cov}(X_1,Y) + \operatorname{cov}(X_2,Y) $
    \item If X and Y are mutually independent, $ \operatorname{cov}(X,Y) = 0 $
\end{itemize}

\subsection{Covariance Matrix}
\[
    \operatorname{cov}(\mathbf{X}) = \Sigma = E[(\mathbf{X} - E[\mathbf{X}])(\mathbf{X} - E[\mathbf{X}])^T]
\]
\textbf{Properties:}
\begin{itemize}
    \item $\operatorname{cov}(A\mathbf{X} + b) = A \, \operatorname{cov}(\mathbf{X}) \, A^T$
\end{itemize}

\subsection{Correlation Coefficient}
\[
    \rho_{X,Y} = \frac{\operatorname{cov}(X,Y)}{\sigma_X\sigma_Y}
\]
\textbf{Properties:}
\begin{itemize}
    \item $ |\rho_{X,Y}| \le 1 $
    \item $ |\rho_{X,Y}| = 1 \Leftrightarrow (\exists a)(\exists b)(\mathbb{P}(Y=aX+b)=1), (a>0, \rho_{X,Y}=1; a<0, \rho_{X,Y}=-1) $
    \item $ \rho_{aX + b, cY + d} = \rho_{X,Y} $
\end{itemize}

\subsection{Markov's Inequality}
If $ X \geq 0 $, for any $ a > 0 $:
\[
    \mathbb{P}(X \geq a) \leq \frac{\mathbb{E}[X]}{a}
\]

\subsection{Chebyshev's Inequality}
For any $ \varepsilon > 0 $:
\[
    \mathbb{P}(|X-\mathbb{E}[X]| \ge \varepsilon) \le \frac{Var(X)}{\varepsilon^2}
\]
\[
    \mathbb{P}(|X-\mathbb{E}[X]| < \varepsilon) \ge 1 - \frac{Var(X)}{\varepsilon^2}
\]

\newpage
\section{One-dimensional Random Variables}

\subsection{Probability Mass Function}
For a discrete random variable, the Probability Mass Function $ p(x) $ is defined as:
\[
    \mathbb{P}(X=x) = p(x)
\]

\subsection{Probability Density Function}
For a continuous random variable, the Probability Density Function $ f(x) $ is defined as:
\[
    \mathbb{P}(a \le x \le b) = \int_{a}^{b} f_X(x)dx
\]

\subsection{Cumulative Distribution Function}
\[
    F(x) = \mathbb{P}(X \le x) = \int_{-\infty}^{x} f_X(t)dt
\]

\subsubsection{Cumulative Distribution Function of a function}
\[
    Y=g(X)
\]
\[
    F(y) = \mathbb{P}(Y \le y) = \int_{g(x) \le y} f_X(x)dx
\]

\subsection{Discrete Probability Distributions}

\subsubsection{Discrete Uniform Distribution}
\[
    X \sim U\{a, a+1, \dots, b\}
\]
\[
    p(x) =
    \begin{cases}
        \frac{1}{n}, & x \in \{a, a+1, \dots, b\} \\
        0, & \text{otherwise}
    \end{cases}
\]
where $n = b - a + 1$.

\[
    \mu = \frac{a + b}{2}
\]
\[
    \sigma^2 = \frac{n^2 - 1}{12}
\]

\subsubsection{Bernoulli Distribution}
Probability of success in a single trial with only two outcomes.
\[ X \sim B(1,p) \]
\[ p(x) = p^x (1-p)^{1-x}, \quad x=0,1 \]
\[ \mu = p \]
\[ \sigma^2 = p(1-p) \]

\subsubsection{Binomial Distribution}
The number of successes in $ n $ independent Bernoulli trails.
\[ X \sim B(n,p) \]
\[ p(x) = \binom{n}{x} p^x (1-p)^{n-x}, \quad x=0,1,\dots,n \]
\[ \mu = np \]
\[ \sigma^2 = np(1-p) \]

\subsubsection{Geometric Distribution}
The number of trials needed to get the first success in a sequence of independent Bernoulli trials.
\[ X \sim G(p) \]
\[ p(x) = (1-p)^{x-1}p, \quad x=1,2,\dots \]
\[ F(x) = 1 - (1-p)^x \]
\[ \mu = \frac{1}{p} \]
\[ \sigma^2 = \frac{1-p}{p^2} \]

\subsubsection{Poisson Distribution}
The number of events occurring in an interval of time, given an average rate $ \lambda $ of occurrence.
\[ X \sim \mathbb{P}(\lambda) \]
\[ p(x) = \frac{\lambda^x e^{-\lambda}}{x!}, \quad \lambda>0, x=0,1,\dots \]
\[ \mu = \sigma^2 = \lambda \]
\textbf{Derivation (Poisson Limit Theorem):}
\[
    B(n, p) \xrightarrow{n \to \infty,\, p \to 0} \mathbb{P}(\lambda = np)
\]

\subsection{Continuous Probability Distributions}

\subsubsection{Uniform Distribution}
\[
    X \sim U(a, b)
\]
\[
    f(x) =
    \begin{cases}
        \frac{1}{b-a}, & a \leq x \leq b \\
        0, & \text{otherwise}
    \end{cases}
\]
\[
    F(x) =
    \begin{cases}
        0, & x < a \\
        \frac{x-a}{b-a}, & a \leq x \leq b \\
        1, & x > b
    \end{cases}
\]
\[
    \mu = \frac{a+b}{2}
\]
\[
    \sigma^2 = \frac{(b-a)^2}{12}
\]

\subsubsection{Exponential Distribution}
The waiting time for the next event in a Poisson process with an average rate $ \lambda $.
\[ X \sim E(\lambda) \]
\[
    f(x) = \lambda e^{-\lambda x}, \quad x \geq 0
\]
\[
    F(x) = 1 - e^{-\lambda x}, \quad x \geq 0
\]
\[
    \mu = \frac{1}{\lambda}
\]
\[
    \sigma^2 = \frac{1}{\lambda^2}
\]
Derivation from Poisson:
\[
    \mathbb{P}(X>x) = f_{\text{Poisson}(\lambda x)}(0)
\]

\subsubsection{Gamma Distribution}
The waiting time for the $ r^{th} $ event in a Poisson process with an average rate $ \lambda $.
\[ X \sim \mathrm{Gamma}(r, \lambda) \]
\[
    f(x) = \frac{\lambda^r x^{r-1} e^{-\lambda x}}{(r-1)!}, \quad x \geq 0
\]
\[
    \mu = \frac{r}{\lambda}
\]
\[
    \sigma^2 = \frac{r}{\lambda^2}
\]
Derivation from Poisson:
\[
    \mathbb{P}(X > x) = \sum_{k=0}^{r-1} f_{\text{Poisson}(\lambda x)}(k)
\]

\subsubsection{Normal Distribution}
\[ X \sim \mathcal{N}(\mu, \sigma^2) \]
\[
    f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right)
\]
\textbf{Derivation (De Moivreâ€“Laplace theorem):}
\[
    B(n, p) \xrightarrow{n \to \infty} \mathcal{N}(\mu = np, \sigma^2 = np(1-p))
\]

\subsubsection{Standard Normal Distribution}
\[ Z = \frac{X-\mu}{\sigma} \sim \mathcal{N}(0,1) \]
\[
    \phi(z) = \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{z^2}{2} \right)
\]
\[
    \Phi(x) = \int_{-\infty}^{x} \phi(t)dt
\]

\newpage
\section{Two-dimensional Continuous Random Variable}

\subsection{Probability Distribution Function}
For two continuous random variables, the Probability Density Function $ f(x,y) $ is defined as:
\[
    \mathbb{P}((X,Y) \in A) = \iint_{A} f(x,y) dx dy
\]

\subsection{Cumulative Distribution Function}
\[
    F(x,y) = \int_{-\infty}^{y} \int_{-\infty}^{x} f(u,v) du dv
\]

\subsection{Marginal Probability Density Function}
\[
    f_X(x) = \int_{-\infty}^{\infty} f(x,y)dy
\]
\[
    f_Y(y) = \int_{-\infty}^{\infty} f(x,y)dx
\]

\subsection{Conditional Probability Density Function}
\[
    f_{X|Y}(x|y) = \frac{f(x,y)}{f_Y(y)}
\]
\[
    f_{Y|X}(y|x) = \frac{f(x,y)}{f_X(x)}
\]

\subsection{Convolution of Probability Distributions}
\[
    Z = X + Y
\]
\[
    f_Z(z) = \int_{-\infty}^{+\infty} f(x,z-x)dx
\]
\[
    f_Z(z) = \int_{-\infty}^{+\infty} f(z-y,y)dy
\]

\subsection{Distribution of a Function of Two Random Variables}
\[
    z=g(x,y)
\]
\[
    y=h(x,z)
\]
\[
    f_Z(z) = \int_{-\infty}^{+\infty} f(x,h(x,z)) \left| \frac{\partial h}{\partial z} \right| dx
\]

\subsection{Bivariate Normal Distribution}
\[ (X,Y) \sim \mathcal{N}(\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, \rho) \]
\[
    f(x_1, x_2) = \frac{1}{2\pi\sqrt{\det(\Sigma)}} \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{x} - \boldsymbol{\mu})\right)
\]

\textbf{Properties:}
\begin{itemize}
    \item $ X \sim \mathcal{N}(\mu_1, \sigma_1^2), Y \sim \mathcal{N}(\mu_2, \sigma_2^2) $
    \item X and Y are mutually independent iff, $ \rho = 0 $
\end{itemize}

\subsection{Standard Bivariate Normal Distribution}
\[
    f(x_1, x_2) = \frac{1}{2\pi} \exp\left(-\frac{x_1^2 + x_2^2}{2}\right)
\]

\newpage
\section{Multivariate Normal Distribution}

\subsection{Mahalanobis Distance}
\[
    \Delta^2 = (\mathbf{x} - \boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{x} - \boldsymbol{\mu})
\]

\paragraph{Apply Spectral Decomposition:}
\[
    \Sigma^{-1} = (Q \Lambda Q^\top)^{-1} = Q \Lambda^{-1} Q^\top
\]
\[
    \Delta^2 = (\mathbf{x} - \boldsymbol{\mu})^\top (Q \Lambda^{-1} Q^\top) (\mathbf{x} - \boldsymbol{\mu})
\]
Define $ \mathbf{y} = Q^\top (\mathbf{x} - \boldsymbol{\mu}) $:
\[
    \Delta^2 = \mathbf{y}^\top \Lambda^{-1} \mathbf{y} = \sum_{i=1}^{d} \frac{y_i^2}{\lambda_i}
\]

\subsection{Multivariate Normal Distribution}
\[ \mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma) \]
\[
    f(\mathbf{x}; \boldsymbol{\mu}, \Sigma) = \frac{1}{\sqrt{(2\pi)^d\det(\Sigma)}} \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{x} - \boldsymbol{\mu})\right)
\]


\newpage
\section{Information Theory}

Let $X$ be a random variable with outcomes in the set $\mathcal{X}$, with $p(x) = \mathbb{P}(X=x)$.

\subsection{Self-Information}
The \textbf{Self-Information} (or surprisal) of an outcome $x \in \mathcal{X}$ is defined as:
\[
    I(x) = -\log_2 p(x)
\]

\subsection{Entropy}
The \textbf{Entropy} of $X$ measures the average uncertainty of the random variable.
\[
    H(X) = \mathbb{E}[I(X)] = \mathbb{E}[-\log_2 p(X)] = -\sum_{x \in \mathcal{X}} p(x) \log_2 p(x)
\]

\subsection{Cross-Entropy}
Let $P$ and $Q$ be two probability distributions over $\mathcal{X}$, with PMFs $p(x)$ and $q(x)$ respectively.
\textbf{Cross-Entropy} measures the average surprisal of events drawn from a true distribution $P$, with a hypothesis distribution $Q$.
\[
    H(P, Q) = \mathbb{E}_p[I_q(X)] = -\sum_{x \in \mathcal{X}} p(x) \log_2 q(x)
\]

\subsection{Kullback-Leibler (KL) Divergence}
The \textbf{KL Divergence} (or relative entropy) of $Q$ from $P$ measures the extra average surprisal caused by the divergence of $Q$ from $P$.
\[
    D_{KL}(P||Q) = H(P, Q) - H(P)
\]
\[
    D_{KL}(P||Q) = \mathbb{E}_p\left[\log_2 \frac{p(X)}{q(X)}\right] = \sum_{x \in \mathcal{X}} p(x) \log_2 \frac{p(x)}{q(x)}
\]

\end{document}
