\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{amssymb}

\title{Statistics}
\date{}

\begin{document}
\maketitle

\tableofcontents

\newpage
\section{Sample Statistics}

\subsection{Sample Mean}
\[
    \overline{X} = \frac{1}{n}\sum_{i=1}^{n} X_i
\]
\textbf{Properties:}
\begin{itemize}
    \item $\mathbb{E}[\overline{X}] = \mu$
    \item $\text{Var}(\overline{X}) = \frac{\sigma^2}{n}$
\end{itemize}

\subsection{Sample Variance}
\[
    S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \overline{X})^2 = \frac{1}{n-1}\left(\sum_{i=1}^{n} X_i^2 - n\overline{X}^2\right)
\]
\textbf{Properties:}
\begin{itemize}
    \item $\mathbb{E}[S^2] = \sigma^2$
\end{itemize}

\subsection{Sample Covariance}
\[
    S_{XY} = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \overline{X})(Y_i - \overline{Y})
\]
\textbf{Properties:}
\begin{itemize}
    \item $\mathbb{E}[S_{XY}] = \operatorname{Cov}(X,Y)$
\end{itemize}

\subsection{Sample Covariance Matrix}
\[
    S = \frac{1}{n-1}\sum_{i=1}^{n}(\mathbf{X}_i - \overline{\mathbf{X}})(\mathbf{X}_i - \overline{\mathbf{X}})^\top
\]
\textbf{Properties:}
\begin{itemize}
    \item $\mathbb{E}[S] = \Sigma$
\end{itemize}

\subsection{Central Limit Theorem}

Let $\{X_i\}_{i=1}^{n}$ be i.i.d. (independent and identically distributed) random variables with mean $\mu$ and variance $\sigma^2$:
\[
    \overline{X}_n \overset{d}{\rightarrow} \mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right) \text{ as } n \rightarrow \infty
\]


\newpage
\section{Sample Distributions}

\subsection{Chi-Square Distribution}
\[
    X \sim \chi^2(n)
\]
where $Z_i \sim \mathcal{N}(0,1)$ and:
\[
    \chi^2 = \sum_{i=1}^{n} Z_i^2
\]
\textbf{Properties:}
\begin{itemize}
    \item $\mathbb{E}[\chi^2] = n$
    \item $\text{Var}(\chi^2) = 2n$
    \item $\chi_1^2 \sim \chi^2(n_1), \chi_2^2 \sim \chi^2(n_2)$: $\chi_1^2 + \chi_2^2 \sim \chi^2(n_1 + n_2)$
    \item $X_i \sim N(\mu, \sigma^2)$: $\chi^2 = \frac{1}{\sigma^2}\sum_{i=1}^{n}(X_i - \mu)^2 \sim \chi^2(n)$
    \item $X_i \sim N(\mu, \sigma^2)$: $\frac{(n-1)S^2}{\sigma^2} = \frac{1}{\sigma^2}\sum_{i=1}^{n}(X_i - \overline{X})^2 \sim \chi^2(n-1)$
\end{itemize}

\subsection{t Distribution}
\[
    t \sim t(n)
\]
where $X \sim \mathcal{N}(0,1)$, $Y \sim \chi^2(n)$, and:
\[
    t = \frac{X}{\sqrt{Y/n}}
\]
\textbf{Properties:}
\begin{itemize}
    \item $n \rightarrow +\infty$: $\mathcal{N}(0,1)$
    \item $T = \frac{\overline{X} - \mu}{S/\sqrt{n}} \sim t(n-1)$
\end{itemize}

\subsection{F Distribution}
\[
    F \sim F(m,n)
\]
where $X \sim \chi^2(m)$, $Y \sim \chi^2(n)$, and:
\[
    F = \frac{X/m}{Y/n}
\]
\textbf{Properties:}
\begin{itemize}
    \item $F_{1-\alpha}(n_1, n_2) = \frac{1}{F_\alpha(n_2, n_1)}$
    \item $t \sim t(n)$: $t^2 \sim F(1, n)$
\end{itemize}


\newpage
\section{Estimation}

\subsection{Moment Estimation}

\subsubsection{Raw Moment}
\[
    \mu_n' = \mathbb{E}[X^n]
\]

\subsubsection{Central Moment}
\[
    \mu_n = \mathbb{E}[(X - \mathbb{E}[X])^n]
\]

\subsubsection{Standardized Moment}
\[
    \gamma_n = \frac{\mu_n}{\sigma^n} = \frac{\mathbb{E}[(X - \mu)^n]}{\sigma^n}
\]

\subsubsection{Relationships Between Parameters and Moments}
\textbf{Mean:}
\[
    \mu = \mu_1'
\]
\textbf{Variance:}
\[
    \sigma^2 = \mu_2 = \mu_2' - (\mu_1')^2
\]
\textbf{Skewness:}
\[
    \gamma_3 = \frac{\mu_3}{\sigma^3} = \frac{\mu_3' - 3\mu_1'\mu_2' + 2(\mu_1')^3}{\sigma^3}
\]

\subsubsection{Moment Generating Function}
The Moment Generating Function is the Laplace Transform of the Probability Density Function:
\[
    M_X(t) = \mathbb{E}[e^{tX}]
\]
\[
    \mu_n' = \left.\frac{d^n}{dt^n} M_X(t)\right|_{t=0}
\]

\subsubsection{Moment Generating Function Series}
\[
    M_X(t) = \mathbb{E}[e^{tX}] = \sum_{n=0}^{\infty} \frac{\mu_n' t^n}{n!}
\]

\subsection{Maximum Likelihood Estimation (MLE)}
Likelihood Function:
\[
    L(\theta) = \prod_{i=1}^{n} p(x_i|\theta)
\]
Maximum Likelihood:
\[
    \hat{\theta} = \arg\max_\theta L(\theta)
\]
Log likelihood:
\[
    \ell(\theta) = \log L(\theta) = \sum_{i=1}^{n} \log p(x_i|\theta)
\]
Found $\hat{\theta}$ by solving:
\[
    \frac{\partial \ell(\theta)}{\partial \theta_i} = 0
\]

\subsubsection{Linear Regression (Ordinary Least Squares)}
For a \textbf{Linear Regression} model in matrix form:
\[
    A\mathbf{x} = \mathbf{b}
\]
where $\mathbf{b} \notin \text{Col}(A)$ and $\boldsymbol{\epsilon} = \mathbf{b} - A\mathbf{x} \sim \mathcal{N}(\mathbf{0}, \sigma^2 I)$.
\newline
By MLE, the optimal approximate solution (set) is found by minimizing the \textbf{Ordinary Least Squares (OLS)}:
\[
    \min_{\mathbf{x}} \|A\mathbf{x} - \mathbf{b}\|^2
\]

\subsubsection{Ridge Regression (OLS + L2 Regularization)}
\textbf{Ridge Regression} is a regularized version of Linear Regression that uses \textbf{L2 Regularization}.
\newline
In Maximum A Posteriori (MAP) estimation framework, assume prior distribution $\mathbf{x} \sim \mathcal{N}(\mathbf{0}, \tau^2 I)$. By MLE, the optimal solution of OLS is found by:
\[
    \min_{\mathbf{x}} \|A\mathbf{x} - \mathbf{b}\|^2 + \lambda\|\mathbf{x}\|_2^2
\]
where the regularization parameter $\displaystyle \lambda = \frac{\sigma^2}{\tau^2}$.


\newpage
\section{Interval Estimation}

For significance level $\alpha$:
\[
    \Pr(\underline{\theta} < \theta < \overline{\theta}) \geq 1 - \alpha
\]

\subsection{Single Population}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Unknown} & \textbf{Known} & \textbf{Distribution} & \textbf{Two-tailed Confidence Interval} \\
\hline
$\mu$ & $\sigma^2$ & $u = \frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \sim \mathcal{N}(0,1)$ & $\left(\overline{X} \pm \frac{\sigma}{\sqrt{n}} u_{\alpha/2}\right)$ \\
\hline
$\mu$ & $S^2$ & $t = \frac{\overline{X} - \mu}{S/\sqrt{n}} \sim t(n-1)$ & $\left(\overline{X} \pm \frac{S}{\sqrt{n}} t_{\alpha/2}(n-1)\right)$ \\
\hline
$\sigma^2$ & $S^2$ & $\chi^2 = \frac{(n-1)S^2}{\sigma^2} \sim \chi^2(n-1)$ & $\left(\frac{(n-1)S^2}{\chi^2_{\alpha/2}(n-1)}, \frac{(n-1)S^2}{\chi^2_{1-\alpha/2}(n-1)}\right)$ \\
\hline
\end{tabular}
\end{table}

\subsection{Two Populations}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Unknown} & \textbf{Known} & \textbf{Distribution} & \textbf{Two-tailed Confidence Interval} \\
\hline
$\mu_1 - \mu_2$ & $\sigma_1^2, \sigma_2^2$ & $\frac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}} \sim \mathcal{N}(0,1)$ & $\left((\overline{X} - \overline{Y}) \pm u_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}\right)$ \\
\hline
$\mu_1 - \mu_2$ & $S_1^2, S_2^2$ & $\frac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}{S_W\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim t(n_1 + n_2 - 2)$ & $\left((\overline{X} - \overline{Y}) \pm t_{\alpha/2}(n_1 + n_2 - 2) S_W\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}\right)$ \\
& & $S_W^2 = \frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1 + n_2 - 2}$ & \\
\hline
$\frac{\sigma_1^2}{\sigma_2^2}$ & $S_1^2, S_2^2$ & $\frac{S_1^2/S_2^2}{\sigma_1^2/\sigma_2^2} \sim F(n_1-1, n_2-1)$ & $\left(\frac{S_1^2/S_2^2}{F_{1-\alpha/2}(n_1-1, n_2-1)}, \frac{S_1^2/S_2^2}{F_{\alpha/2}(n_1-1, n_2-1)}\right)$ \\
\hline
\end{tabular}
\end{table}

\section{Principal Component Analysis (PCA)}

\subsection{Best Fitting Subspace (BFS)}

\begin{enumerate}
    \item \textbf{Objective:}
    \[
        \arg\min_{W} \text{cost}(W)
    \]
    where \textbf{Cost Function:}
    \[
        \text{cost}(W) = \sum_{i=1}^{n} \|\mathbf{a}_i - P_W \mathbf{a}_i\|^2
    \]

    \item By the \textbf{Pythagorean Theorem:}
    \[
        \|\mathbf{a}_i - P_W \mathbf{a}_i\|^2 = \|\mathbf{a}_i\|^2 - \|P_W \mathbf{a}_i\|^2
    \]
    \[
        \sum_{i=1}^{n} \|\mathbf{a}_i - P_W \mathbf{a}_i\|^2 = \sum_{i=1}^{n} \|\mathbf{a}_i\|^2 - \sum_{i=1}^{n} \|P_W \mathbf{a}_i\|^2
    \]

    \item Let $\{\mathbf{q}_j\}_{j=1}^{k}$ be an \textbf{orthonormal basis} for $W$.
    By \textbf{Parseval's Identity:}
    \[
        \|P_W \mathbf{a}_i\|^2 = \sum_{j=1}^{k} \langle \mathbf{a}_i, \mathbf{q}_j \rangle^2
    \]
    \[
        \sum_{i=1}^{n} \|P_W \mathbf{a}_i\|^2
        = \sum_{i=1}^{n} \sum_{j=1}^{k} \langle \mathbf{a}_i, \mathbf{q}_j \rangle^2
        = \sum_{j=1}^{k} \sum_{i=1}^{n} \langle \mathbf{a}_i, \mathbf{q}_j \rangle^2
    \]

    \item Define Matrix $A$:
    \[
        A =
        \begin{bmatrix}
            - \mathbf{a}_1^T - \\
            \vdots \\
            - \mathbf{a}_n^T -
        \end{bmatrix}
    \]
    \[
        \sum_{i=1}^{n} \langle \mathbf{a}_i, \mathbf{q}_j \rangle^2 = \|A\mathbf{q}_j\|^2 = \mathbf{q}_{j}^T A^T A \mathbf{q}_j
    \]

    \item New Objective Function:
    \[
        \arg\min_{W} \text{cost}(W) = \arg\max_{W} \sum_{i=1}^{k} \mathbf{q}_i^T A^T A \mathbf{q}_i
    \]

    \item \textbf{Greedy Algorithm:}
    \newline
    Object: \textbf{Conditional Maximization:}
    \[
        \arg\max_{\|\mathbf{q}_i\|=1} \mathbf{q}_i^T A^T A \mathbf{q}_i
    \]
    By \textbf{Lagrange Multiplier Method}:
    \[
        \mathcal{L}(\mathbf{q}_i, \lambda) = \mathbf{q}_i^T A^T A \mathbf{q}_i - \lambda(\mathbf{q}_i^T \mathbf{q}_i - 1)
    \]
    \[
        \frac{\partial \mathcal{L}}{\partial \mathbf{q}_i} = 2 A^T A \mathbf{q}_i - 2\lambda\mathbf{q}_i = \mathbf{0}
    \]
    \[
        A^T A \mathbf{q}_i = \lambda\mathbf{q}_i
    \]
    Solutions:
    \begin{itemize}
        \item $\mathbf{q}_i = \mathbf{v}_i$ is an \textbf{Eigenvector} of $A^T A$
        \item $\lambda = \lambda_i$ is an \textbf{Eigenvalue} of $A^T A$
        \item $\mathbf{q}_i = \mathbf{v}_i$ is a \textbf{Right Singular Vector} of $A$
        \item $\sigma_i = \sqrt{\lambda_i}$ is a \textbf{Singular Value} of $A$
    \end{itemize}
    \[
        \mathbf{q}_i^T A^T A \mathbf{q}_i = \mathbf{v}_i^T \lambda_i \mathbf{v}_i = \lambda_i = \sigma_i^2
    \]

    \item Therefore:
    \[
        \arg\min_{W} \text{cost}(W) = \arg\max_{W} \sum_{j=1}^{k} \sigma_i^2
    \]
    \[
        W = \text{span}(\{\mathbf{v}_i\}_{i=1}^{k})
    \]
\end{enumerate}

\subsection{PCA}

\textbf{Objective:} Find the $k$-BFS for centered data.
\newline
\textbf{Data matrix} $X \in \mathbb{R}^{n \times d}$:
\[
    X =
    \begin{bmatrix}
        - \mathbf{x}_1^T - \\
        \vdots \\
        - \mathbf{x}_n^T -
    \end{bmatrix}
\]
\textbf{Centered Data Matrix} $B$:
\[
    B = X - \overline{X}
\]
\textbf{Principal Components} $\mathbf{v}_i$ are the \textbf{Right Singular Vectors} of $B$.
\[
    W = \text{span}(\{\mathbf{v}_i\}_{i=1}^{k})
\]
\textbf{Scores} are the data projected onto \textbf{Principal Components}.
\[
    T = BV
\]
\newline
\textbf{Variance:}
\[
    \text{Var}(B\mathbf{v}_i) = \frac{1}{n-1} \|B\mathbf{v}_i\|^2
\]
\[
    \arg\min_{W} \text{cost}(W) = \arg\max_{W} \sum_{i=1}^{k} \text{Var}(B\mathbf{v}_i)
\]
\textbf{Sample Covariance Matrix:}
\[
    S = \frac{1}{n-1} B^T B
\]
Let $\lambda_i$ be an \textbf{Eigenvalue} of $S$, $\sigma_i$ be an \textbf{Right Singular Value} of $B$.
\[
    \lambda_i = \frac{\sigma_i^2}{n-1}
\]


\end{document}
