\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{amssymb}
\usepackage{mathtools}

\title{Statistics}
\date{}

\begin{document}
\maketitle

\tableofcontents

\newpage
\section{Sample Statistics}

\subsection{Sample Mean}
\[
    \overline{X} = \frac{1}{n}\sum_{i=1}^{n} X_i
\]
\textbf{Properties:}
\begin{itemize}
    \item $\mathbb{E}[\overline{X}] = \mu$
    \item $\text{Var}(\overline{X}) = \frac{\sigma^2}{n}$
\end{itemize}

\subsection{Sample Variance}
\[
    S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \overline{X})^2 = \frac{1}{n-1}\left(\sum_{i=1}^{n} X_i^2 - n\overline{X}^2\right)
\]
\textbf{Properties:}
\begin{itemize}
    \item $\mathbb{E}[S^2] = \sigma^2$
\end{itemize}

\subsection{Sample Covariance}
\[
    Cov(X,Y) = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \overline{X})(Y_i - \overline{Y})
\]
\textbf{Properties:}
\begin{itemize}
    \item $\mathbb{E}[Cov(X,Y)] = \operatorname{cov}(X,Y)$
\end{itemize}

\subsection{Sample Covariance Matrix}
\[
    S = \frac{1}{n-1}\sum_{i=1}^{n}(\mathbf{X}_i - \overline{\mathbf{X}})(\mathbf{X}_i - \overline{\mathbf{X}})^\top
\]
\textbf{Properties:}
\begin{itemize}
    \item $\mathbb{E}[S] = \Sigma$
\end{itemize}

\subsection{Central Limit Theorem}

Let $\{X_i\}_{i=1}^{n}$ be i.i.d. (independent and identically distributed) random variables with mean $\mu$ and variance $\sigma^2$:
\[
    \overline{X}_n \overset{d}{\rightarrow} \mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right) \text{ as } n \rightarrow \infty
\]


\newpage
\section{Sample Distributions}

\subsection{Chi-Square Distribution}
\[
    X \sim \chi^2(n)
\]
where $Z_i \sim \mathcal{N}(0,1)$ and:
\[
    \chi^2 = \sum_{i=1}^{n} Z_i^2
\]
\textbf{Properties:}
\begin{itemize}
    \item $\mathbb{E}[\chi^2] = n$
    \item $\text{Var}(\chi^2) = 2n$
    \item $\chi_1^2 \sim \chi^2(n_1), \chi_2^2 \sim \chi^2(n_2)$: $\chi_1^2 + \chi_2^2 \sim \chi^2(n_1 + n_2)$
    \item $X_i \sim N(\mu, \sigma^2)$: $\chi^2 = \frac{1}{\sigma^2}\sum_{i=1}^{n}(X_i - \mu)^2 \sim \chi^2(n)$
    \item $X_i \sim N(\mu, \sigma^2)$: $\frac{(n-1)S^2}{\sigma^2} = \frac{1}{\sigma^2}\sum_{i=1}^{n}(X_i - \overline{X})^2 \sim \chi^2(n-1)$
\end{itemize}

\subsection{t Distribution}
\[
    t \sim t(n)
\]
where $X \sim \mathcal{N}(0,1)$, $Y \sim \chi^2(n)$, and:
\[
    t = \frac{X}{\sqrt{Y/n}}
\]
\textbf{Properties:}
\begin{itemize}
    \item $n \rightarrow +\infty$: $\mathcal{N}(0,1)$
    \item $T = \frac{\overline{X} - \mu}{S/\sqrt{n}} \sim t(n-1)$
\end{itemize}

\subsection{F Distribution}
\[
    F \sim F(m,n)
\]
where $X \sim \chi^2(m)$, $Y \sim \chi^2(n)$, and:
\[
    F = \frac{X/m}{Y/n}
\]
\textbf{Properties:}
\begin{itemize}
    \item $F_{1-\alpha}(n_1, n_2) = \frac{1}{F_\alpha(n_2, n_1)}$
    \item $t \sim t(n)$: $t^2 \sim F(1, n)$
\end{itemize}


\newpage
\section{Estimation}

\subsection{Maximum Likelihood Estimation (MLE)}
Likelihood Function:
\[
    L_i(\theta) = \mathbb{P}(x|\theta)
\]
\[
    L(\theta|X) = \prod_{i=1}^{n} \mathbb{P}(x_i|\theta)
\]
Log Likelihood:
\[
    \ell(\theta) = \log L(\theta) = \log \mathbb{P}(x|\theta)
\]
\[
    \ell(\theta|X) = \log L(\theta|X) = \sum_{i=1}^{n} \log \mathbb{P}(x_i|\theta)
\]
Maximum Log Likelihood:
\[
    \hat{\theta} = \arg\max_\theta \ell(\theta | X)
\]

\subsubsection{On Bernoulli Distribution}
\[
    y \sim \text{Bernoulli}(\hat{y})
\]
Likelihood Function:
\[
    L(\theta) = \hat{y}^y (1-\hat{y})^{1-y}
\]
Log Likelihood:
\[
    \ell(\theta) = y \log \hat{y} + (1-y) \log (1-\hat{y})
\]
Maximum Log Likelihood:
\[
    \hat{\theta} = \arg\max_{\theta} \left( y \log \hat{y} + (1-y) \log (1-\hat{y}) \right)
\]

\subsubsection{On Categorical Distribution}
\[
    \mathbf{y} \sim \text{Categorical}(\hat{\mathbf{y}})
\]
Likelihood Function:
\[
    L(\theta) = \prod_{k=1}^{K} \hat{y}_k^{y_k}
\]
Log Likelihood:
\[
    \ell(\theta) = \sum_{k=1}^{K} y_k \log \hat{y}_k = \mathbf{y}^\top \log \hat{\mathbf{y}}
\]
Maximum Log Likelihood:
\[
    \hat{\theta} = \arg\max_{\theta} \mathbf{y}^\top \log \hat{\mathbf{y}}
\]

\subsubsection{On Gaussian Distribution}
\[
    y \sim \mathcal{N}(\hat{y}, \sigma^2)
\]
Likelihood Function:
\[
    L(\theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y-\hat{y})^2}{2\sigma^2}\right)
\]
Log Likelihood:
\[
    \ell(\theta) = -\frac{1}{2}\log(2\pi\sigma^2) - \frac{(y-\hat{y})^2}{2\sigma^2}
\]
Maximum Log Likelihood:
\[
    \hat{\theta} = \arg\min_{\theta} (y-\hat{y})^2
\]


\subsection{Maximum A Posteriori (MAP)}

\[
    \hat{\theta}_{\text{MAP}} = \arg\max_{\theta} \mathbb{P}(\theta | X) = \arg\max_{\theta} \frac{\mathbb{P}(X | \theta) \mathbb{P}(\theta)}{\mathbb{P}(X)} = \arg\max_{\theta} \left( \log \mathbb{P}(X | \theta) + \log \mathbb{P}(\theta) \right)
\]
\[
    \hat{\theta}_{\text{MAP}} = \arg\max_{\theta} \left( \ell(\theta) + \log \mathbb{P}(\theta) \right)
\]

\subsubsection{On Gaussian Prior}
\[
    \theta \sim \mathcal{N}(\mathbf{0}, \tau^2 I)
\]
\[
    \hat{\theta}_{\text{MAP}} = \arg\max_{\theta} \left( \ell(\theta) - \frac{1}{2\tau^2} \|\theta\|_2^2 \right)
\]


\subsection{Max Entropy}

\paragraph{Sample:}
\[
    \mathbf{x} \in \mathcal{X} = \mathbb{R}^D, \quad \mathbf{y} \in \mathcal{Y}, \quad \mathcal{S} = \left( (\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) \right)_{i=1}^N
\]
\paragraph{Marginal Distribution:}
\[
    p(\mathbf{y}|\mathbf{x}) = \mathbb{P}(\mathbf{Y}=\mathbf{y}|\mathbf{X}=\mathbf{x})
\]
\paragraph{Sufficient Statistics:}
\[
    \mathbf{T}: \mathcal{Y} \rightarrow \mathbb{R}^M
\]
\paragraph{Normalization Constraint:}
\[
    \sum_{\mathbf{y}} p(\mathbf{y}|\mathbf{x}) = 1
\]

\subsubsection{Local Constraint}

\paragraph{Principal of Maximum Entropy:}
\[
    H(\mathbf{Y}|\mathbf{X}=\mathbf{x}) = - \sum_{\mathbf{y}} p(\mathbf{y}|\mathbf{x}) \ln p(\mathbf{y}|\mathbf{x})
\]
\paragraph{Moment Matching:} \mbox{}
\newline
Local Constraint:
\[
    \mathbb{E}[\mathbf{T}(\mathbf{Y})|\mathbf{x}] = \hat{\mathbb{E}}[\mathbf{T}(\mathbf{Y})|\mathbf{x}] \in \mathbb{R}^M
\]
\[
    \sum_{\mathbf{y}} p(\mathbf{y}|\mathbf{x}) \mathbf{T}(\mathbf{y}) = \hat{\mathbb{E}}[\mathbf{T}(\mathbf{Y})|\mathbf{x}]
\]
\paragraph{Lagrange Multiplier:}
\begin{align*}
    \mathcal{L}(p, \alpha, \boldsymbol{\lambda}) &= - \sum_{\mathbf{y}} p(\mathbf{y}|\mathbf{x}) \ln p(\mathbf{y}|\mathbf{x}) + \alpha(\mathbf{x}) \left(\sum_{\mathbf{y}} p(\mathbf{y}|\mathbf{x}) - 1\right) \\
    &\quad + \left\langle \boldsymbol{\lambda}(\mathbf{x}), \sum_{\mathbf{y}} p(\mathbf{y}|\mathbf{x}) \mathbf{T}(\mathbf{y}) - \hat{\mathbb{E}}[\mathbf{T}(\mathbf{Y})|\mathbf{x}] \right\rangle
\end{align*}
where $\boldsymbol{\lambda} \in \mathbb{R}^M$.
\[
    \frac{\partial \mathcal{L}}{\partial p(\mathbf{y}|\mathbf{x})} = - (1 + \ln p(\mathbf{y}|\mathbf{x})) + \alpha(\mathbf{x}) + \boldsymbol{\lambda}(\mathbf{x})^{\mathrm{T}} \mathbf{T}(\mathbf{y}) = 0
\]
\[
    p(\mathbf{y}|\mathbf{x}) = \exp(\alpha(\mathbf{x}) - 1) \cdot \exp(\boldsymbol{\lambda}(\mathbf{x})^{\mathrm{T}} \mathbf{T}(\mathbf{y}))
\]
\[
    p(\mathbf{y}|\mathbf{x}) = \frac{\exp(\boldsymbol{\lambda}(\mathbf{x})^{\mathrm{T}} \mathbf{T}(\mathbf{y}))}{\sum_{\mathbf{y} \in \mathcal{Y}} \exp(\boldsymbol{\lambda}(\mathbf{x})^{\mathrm{T}} \mathbf{T}(\mathbf{y}))}
\]
\paragraph{Moment Matching:}
\[
    \mathbb{E}[\mathbf{T}(\mathbf{Y})|\mathbf{x}] = \hat{\mathbb{E}}[\mathbf{T}(\mathbf{Y})|\mathbf{x}]
\]
\[
    \sum_{\mathbf{y} \in \mathcal{Y}} p(\mathbf{y}|\mathbf{x}^{(i)}) \mathbf{T}(\mathbf{y}) = \mathbf{T}(\mathbf{y}^{(i)})
\]

\subsubsection{Global Constraint}

\paragraph{Principal of Maximum Entropy:}
\[
    H(\mathbf{Y}|\mathbf{X}) = \sum_\mathbf{x} \tilde{p}(\mathbf{x}) H(\mathbf{Y}|\mathbf{X}=\mathbf{x}) = - \sum_\mathbf{x} \tilde{p}(\mathbf{x}) \sum_{\mathbf{y}} p(\mathbf{y}|\mathbf{x}) \ln p(\mathbf{y}|\mathbf{x})
\]
\paragraph{Moment Matching:} \mbox{}
\newline
Global Constraint:
\[
    \mathbb{E}[\mathbf{T}(\mathbf{Y})\mathbf{X}^{\mathrm{T}}] = \hat{\mathbb{E}}[\mathbf{T}(\mathbf{Y})\mathbf{X}^{\mathrm{T}}] \in \mathbb{R}^{M \times D}
\]
\[
    \sum_\mathbf{x} \tilde{p}(\mathbf{x}) \sum_{\mathbf{y}} p(\mathbf{y}|\mathbf{x}) \mathbf{T}(\mathbf{y})\mathbf{x}^{\mathrm{T}} = \hat{\mathbb{E}}[\mathbf{T}(\mathbf{Y})\mathbf{X}^{\mathrm{T}}]
\]
Marginal Constraint:
\[
    \mathbb{E}[\mathbf{T}(\mathbf{Y})] = \hat{\mathbb{E}}[\mathbf{T}(\mathbf{Y})] \in \mathbb{R}^M
\]
\[
    \sum_\mathbf{x} \tilde{p}(\mathbf{x}) \sum_{\mathbf{y}} p(\mathbf{y}|\mathbf{x}) \mathbf{T}(\mathbf{y}) = \hat{\mathbb{E}}[\mathbf{T}(\mathbf{Y})]
\]
\paragraph{Lagrange Multiplier:}
\begin{align*}
    \mathcal{L}(p, \alpha, W, \mathbf{b}) &= - \sum_\mathbf{x} \tilde{p}(\mathbf{x}) \sum_{\mathbf{y}} p(\mathbf{y}|\mathbf{x}) \ln p(\mathbf{y}|\mathbf{x}) + \sum_\mathbf{x} \alpha(\mathbf{x}) \left(\sum_{\mathbf{y}} p(\mathbf{y}|\mathbf{x}) - 1\right) \\
    &\quad + \left\langle W, \sum_\mathbf{x} \tilde{p}(\mathbf{x}) \sum_{\mathbf{y}} p(\mathbf{y}|\mathbf{x}) \mathbf{T}(\mathbf{y})\mathbf{x}^{\mathrm{T}} - \hat{\mathbb{E}}[\mathbf{T}(\mathbf{Y})\mathbf{X}^{\mathrm{T}}] \right\rangle \\
    &\quad + \left\langle \mathbf{b}, \sum_\mathbf{x} \tilde{p}(\mathbf{x}) \sum_{\mathbf{y}} p(\mathbf{y}|\mathbf{x}) \mathbf{T}(\mathbf{y}) - \hat{\mathbb{E}}[\mathbf{T}(\mathbf{Y})] \right\rangle
\end{align*}
where $W \in \mathbb{R}^{M \times D}$, $\mathbf{b} \in \mathbb{R}^M$.
\[
    \frac{\partial \mathcal{L}}{\partial p(\mathbf{y}|\mathbf{x})} = - \tilde{p}(\mathbf{x}) (1 + \ln p(\mathbf{y}|\mathbf{x})) + \alpha(\mathbf{x}) + \tilde{p}(\mathbf{x}) \langle W, \mathbf{T}(\mathbf{y})\mathbf{x}^{\mathrm{T}} \rangle + \tilde{p}(\mathbf{x}) \langle \mathbf{b}, \mathbf{T}(\mathbf{y}) \rangle
\]
\[
    p(\mathbf{y}|\mathbf{x}) = \exp\left(\frac{\alpha(\mathbf{x})}{\tilde{p}(\mathbf{x})} - 1\right) \cdot \exp((W\mathbf{x}+\mathbf{b})^{\mathrm{T}} \mathbf{T}(\mathbf{y}))
\]
\[
    \begin{dcases}
        \mathbf{z} = W\mathbf{x}+\mathbf{b} \\
        p(\mathbf{y}|\mathbf{x}) = \frac{\exp(\mathbf{z}^{\mathrm{T}} \mathbf{T}(\mathbf{y}))}{\sum_{\mathbf{y} \in \mathcal{Y}} \exp(\mathbf{z}^{\mathrm{T}} \mathbf{T}(\mathbf{y}))}
    \end{dcases}
\]
\paragraph{Moment Matching:}
\[
    \begin{dcases}
        \mathbb{E}[\mathbf{T}(\mathbf{Y})\mathbf{X}^{\mathrm{T}}] = \hat{\mathbb{E}}[\mathbf{T}(\mathbf{Y})\mathbf{X}^{\mathrm{T}}] \\
        \mathbb{E}[\mathbf{T}(\mathbf{Y})] = \hat{\mathbb{E}}[\mathbf{T}(\mathbf{Y})]
    \end{dcases}
\]
\[
    \begin{dcases}
        \sum_{i=1}^N \left(\sum_{\mathbf{y} \in \mathcal{Y}} p(\mathbf{y}|\mathbf{x}^{(i)}) \mathbf{T}(\mathbf{y}) - \mathbf{T}(\mathbf{y}^{(i)})\right) \mathbf{x}^{(i)\mathrm{T}} = \mathbf{0} \\
        \sum_{i=1}^N \left(\sum_{\mathbf{y} \in \mathcal{Y}} p(\mathbf{y}|\mathbf{x}^{(i)}) \mathbf{T}(\mathbf{y}) - \mathbf{T}(\mathbf{y}^{(i)})\right) = \mathbf{0}
    \end{dcases}
\]

\subsubsection{On Bernoulli Distribution}
\[
    y \sim \text{Bernoulli}(p), \quad \mathcal{Y} = \{0, 1\}
\]
\[
    T(y) = y \in \mathbb{R}, \quad W \in \mathbb{R}^{1 \times D}, \quad b \in \mathbb{R}
\]
\[
    p(Y=1|\mathbf{x}) = \frac{1}{1 + \exp(-z)}
\]
\paragraph{Moment Matching:}
\[
    \begin{dcases}
        \hat{y}^{(i)} = \sum_{y \in \mathcal{Y}} y \cdot p(y|\mathbf{x}^{(i)}) = p(Y=1|\mathbf{x}^{(i)}) \\
        \sum_{i=1}^N (\hat{y}^{(i)} - y^{(i)}) \mathbf{x}^{(i)\mathrm{T}} = \mathbf{0} \\
        \sum_{i=1}^N (\hat{y}^{(i)} - y^{(i)}) = 0
    \end{dcases}
\]

\subsubsection{On Categorical Distribution}
\[
    \mathbf{y} \sim \text{Categorical}(\mathbf{p}), \quad \mathcal{Y} = \{\mathbf{e}_i\}_{i=1}^{K} \subset \{0, 1\}^K
\]
\[
    \mathbf{T}(\mathbf{y}) = \mathbf{y} \in \mathbb{R}^K, \quad W \in \mathbb{R}^{K \times D}, \quad \mathbf{b} \in \mathbb{R}^K
\]
\[
    p(Y=\mathbf{y}|\mathbf{x}) = \frac{\exp(\mathbf{z}^{\mathrm{T}} \mathbf{y})}{\sum_{k=1}^K \exp(\mathbf{z}_k)}
\]
\paragraph{Moment Matching:}
\[
    \begin{dcases}
        \hat{\mathbf{y}}^{(i)} = \sum_{\mathbf{y} \in \mathcal{Y}} \mathbf{y} \cdot p(\mathbf{y}|\mathbf{x}^{(i)}) = \begin{bmatrix} p(\mathbf{Y}=\mathbf{e}_1|\mathbf{x}^{(i)}) \\ \vdots \\ p(\mathbf{Y}=\mathbf{e}_K|\mathbf{x}^{(i)}) \end{bmatrix} \\
        \sum_{i=1}^N (\hat{\mathbf{y}}^{(i)} - \mathbf{y}^{(i)}) \mathbf{x}^{(i)\mathrm{T}} = \mathbf{0} \\
        \sum_{i=1}^N (\hat{\mathbf{y}}^{(i)} - \mathbf{y}^{(i)}) = \mathbf{0}
    \end{dcases}
\]

\subsubsection{On Gaussian Distribution}
\[
    y \sim \mathcal{N}(\mu, \sigma^2), \quad \mathcal{Y} = \mathbb{R}
\]
\[
    \mathbf{T}(y) = \begin{bmatrix} y \\ y^2 \end{bmatrix} \in \mathbb{R}^2, \quad W \in \mathbb{R}^{2 \times D}, \quad \mathbf{b} \in \mathbb{R}^2
\]
\[
    p(Y=y|\mathbf{x}) = \frac{\exp(z_1 y + z_2 y^2)}{\int_{-\infty}^{\infty} \exp(z_1 y' + z_2 (y')^2) dy'} = \sqrt{\frac{-z_2}{\pi}} \exp\left(z_2\left(y + \frac{z_1}{2z_2}\right)^2\right) \quad (z_2 < 0 \text{ for convergence})
\]
\paragraph{Moment Matching:}
\[
    \begin{dcases}
        \hat{y}^{(i)} = \int_{-\infty}^{\infty} y \cdot p(y|\mathbf{x}^{(i)}) dy \\
        \widehat{y^2}^{(i)} = \int_{-\infty}^{\infty} y^2 \cdot p(y|\mathbf{x}^{(i)}) dy \\
        \sum_{i=1}^N \left( \hat{y}^{(i)} - y^{(i)} \right) \mathbf{x}^{(i)} = \mathbf{0} \\
        \sum_{i=1}^N \left( \widehat{y^2}^{(i)} - (y^{(i)})^2 \right) \mathbf{x}^{(i)} = \mathbf{0} \\
        \sum_{i=1}^N \left( \hat{y}^{(i)} - y^{(i)} \right) = 0 \\
        \sum_{i=1}^N \left( \widehat{y^2}^{(i)} - (y^{(i)})^2 \right) = 0
    \end{dcases}
\]
\paragraph{Parameterization:}
\[
    \begin{dcases}
        \hat{y}^{(i)} = \mu(\mathbf{x}^{(i)}) \\
        \widehat{y^2}^{(i)} = \sigma^2(\mathbf{x}^{(i)}) + \mu(\mathbf{x}^{(i)})^2 \\
    \end{dcases}
\]
\[
    \begin{dcases}
        \sigma^2 = -\frac{1}{2z_2} \\
        \mu = -\frac{z_1}{2z_2} \\
        \sum_{i=1}^N \left( \mu(\mathbf{x}^{(i)}) - y^{(i)} \right) \mathbf{x}^{(i)} = \mathbf{0} \\
        \sum_{i=1}^N \left( \sigma^2(\mathbf{x}^{(i)}) + \mu(\mathbf{x}^{(i)})^2 - (y^{(i)})^2 \right) \mathbf{x}^{(i)} = \mathbf{0} \\
        \sum_{i=1}^N \left( \mu(\mathbf{x}^{(i)}) - y^{(i)} \right) = 0 \\
        \sum_{i=1}^N \left( \sigma^2(\mathbf{x}^{(i)}) + \mu(\mathbf{x}^{(i)})^2 - (y^{(i)})^2 \right) = 0
    \end{dcases}
\]

\subsubsection{On Homoscedastic Gaussian Distribution}
\[
    \mathbf{w}_2 = 0
\]
\[
    p(Y=y|\mathbf{x}) = \sqrt{\frac{-b_2}{\pi}} \exp\left( b_2 \left( y + \frac{\mathbf{w}_1^\top \mathbf{x} + b_1}{2b_2} \right)^2 \right)
\]
\paragraph{Parameterization:}
\[
    \begin{dcases}
        \sigma^2 = -\frac{1}{2b_2} \\
        \mu(\mathbf{x}) = -\frac{\mathbf{w}_1^\top \mathbf{x} + b_1}{2b_2} \\
        \sum_{i=1}^N \left( \mu(\mathbf{x}^{(i)}) - y^{(i)} \right) \mathbf{x}^{(i)} = \mathbf{0} \\
        \sum_{i=1}^N \left( \mu(\mathbf{x}^{(i)}) - y^{(i)} \right) = 0 \\
        \sigma^2 = \frac{1}{N} \sum_{i=1}^N (y^{(i)} - \mu(\mathbf{x}^{(i)}))^2
    \end{dcases}
\]
\paragraph{Reparameterization:}
\[
    \begin{dcases}
        \boldsymbol{\beta} = -\frac{\mathbf{w}_1}{2b_2} \\
        \beta_0 = -\frac{b_1}{2b_2}
    \end{dcases}
\]
\[
    \begin{dcases}
        \mu(\mathbf{x}) = \boldsymbol{\beta}^\top \mathbf{x} + \beta_0 \\
        \sum_{i=1}^N \left( \mu(\mathbf{x}^{(i)}) - y^{(i)} \right) \mathbf{x}^{(i)} = \mathbf{0} \\
        \sum_{i=1}^N \left( \mu(\mathbf{x}^{(i)}) - y^{(i)} \right) = 0 \\
        \sigma^2 = \frac{1}{N} \sum_{i=1}^N (y^{(i)} - \mu(\mathbf{x}^{(i)}))^2
    \end{dcases}
\]

\paragraph{Matrix Form:}
\[
    \tilde{\mathbf{x}}^{(i)} = \begin{bmatrix} \mathbf{x}^{(i)} \\ 1 \end{bmatrix} \in \mathbb{R}^{D+1}, \quad
    \tilde{\boldsymbol{\beta}} = \begin{bmatrix} \boldsymbol{\beta} \\ \beta_0 \end{bmatrix} \in \mathbb{R}^{D+1}
\]
\[
    \mathbf{X} =
    \begin{bmatrix}
        - \tilde{\mathbf{x}}^{(1)\top} - \\
        \vdots \\
        - \tilde{\mathbf{x}}^{(N)\top} -
    \end{bmatrix} \in \mathbb{R}^{N \times (D+1)}, \quad
    \mathbf{y} =
    \begin{bmatrix}
        y^{(1)} \\
        \vdots \\
        y^{(N)}
    \end{bmatrix} \in \mathbb{R}^N
\]
\[
    \begin{dcases}
        \mathbf{X}^{\mathrm{T}} (\mathbf{X} \tilde{\boldsymbol{\beta}} - \mathbf{y}) = \mathbf{0} \\
        \boldsymbol{\mu} = \mathbf{X} \tilde{\boldsymbol{\beta}} \\
        \sigma^2 = \frac{1}{N} \|\mathbf{y} - \boldsymbol{\mu}\|^2
    \end{dcases}
\]


\subsection{Interval Estimation}

For significance level $\alpha$:
\[
    \mathbb{P}(\underline{\theta} < \theta < \overline{\theta}) \geq 1 - \alpha
\]

\subsubsection{Single Population}

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Unknown} & \textbf{Known} & \textbf{Distribution} & \textbf{Two-tailed Confidence Interval} \\
\hline
$\mu$ & $\sigma^2$ & $u = \frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \sim \mathcal{N}(0,1)$ & $\left(\overline{X} \pm \frac{\sigma}{\sqrt{n}} u_{\alpha/2}\right)$ \\
\hline
$\mu$ & $S^2$ & $t = \frac{\overline{X} - \mu}{S/\sqrt{n}} \sim t(n-1)$ & $\left(\overline{X} \pm \frac{S}{\sqrt{n}} t_{\alpha/2}(n-1)\right)$ \\
\hline
$\sigma^2$ & $S^2$ & $\chi^2 = \frac{(n-1)S^2}{\sigma^2} \sim \chi^2(n-1)$ & $\left(\frac{(n-1)S^2}{\chi^2_{\alpha/2}(n-1)}, \frac{(n-1)S^2}{\chi^2_{1-\alpha/2}(n-1)}\right)$ \\
\hline
\end{tabular}
\end{center}

\subsubsection{Two Populations}

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Unknown} & \textbf{Known} & \textbf{Distribution} & \textbf{Two-tailed Confidence Interval} \\
\hline
$\mu_1 - \mu_2$ & $\sigma_1^2, \sigma_2^2$ & $\frac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}} \sim \mathcal{N}(0,1)$ & $\left((\overline{X} - \overline{Y}) \pm u_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}\right)$ \\
\hline
$\mu_1 - \mu_2$ & $S_1^2, S_2^2$ & $\frac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}{S_W\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim t(n_1 + n_2 - 2)$ & $\left((\overline{X} - \overline{Y}) \pm t_{\alpha/2}(n_1 + n_2 - 2) S_W\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}\right)$ \\
& & $S_W^2 = \frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1 + n_2 - 2}$ & \\
\hline
$\frac{\sigma_1^2}{\sigma_2^2}$ & $S_1^2, S_2^2$ & $\frac{S_1^2/S_2^2}{\sigma_1^2/\sigma_2^2} \sim F(n_1-1, n_2-1)$ & $\left(\frac{S_1^2/S_2^2}{F_{1-\alpha/2}(n_1-1, n_2-1)}, \frac{S_1^2/S_2^2}{F_{\alpha/2}(n_1-1, n_2-1)}\right)$ \\
\hline
\end{tabular}
\end{center}


\newpage
\section{Principal Component Analysis (PCA)}

\subsection{Best Fitting Subspace (BFS)}

\begin{enumerate}
    \item \textbf{Objective:}
    \[
        \arg\min_{W} \text{cost}(W)
    \]
    where \textbf{Cost Function:}
    \[
        \text{cost}(W) = \sum_{i=1}^{n} \|\mathbf{a}_i - P_W \mathbf{a}_i\|^2
    \]

    \item By the \textbf{Pythagorean Theorem:}
    \[
        \|\mathbf{a}_i - P_W \mathbf{a}_i\|^2 = \|\mathbf{a}_i\|^2 - \|P_W \mathbf{a}_i\|^2
    \]
    \[
        \sum_{i=1}^{n} \|\mathbf{a}_i - P_W \mathbf{a}_i\|^2 = \sum_{i=1}^{n} \|\mathbf{a}_i\|^2 - \sum_{i=1}^{n} \|P_W \mathbf{a}_i\|^2
    \]

    \item Let $\{\mathbf{q}_j\}_{j=1}^{k}$ be an \textbf{orthonormal basis} for $W$.
    By \textbf{Parseval's Identity:}
    \[
        \|P_W \mathbf{a}_i\|^2 = \sum_{j=1}^{k} \langle \mathbf{a}_i, \mathbf{q}_j \rangle^2
    \]
    \[
        \sum_{i=1}^{n} \|P_W \mathbf{a}_i\|^2
        = \sum_{i=1}^{n} \sum_{j=1}^{k} \langle \mathbf{a}_i, \mathbf{q}_j \rangle^2
        = \sum_{j=1}^{k} \sum_{i=1}^{n} \langle \mathbf{a}_i, \mathbf{q}_j \rangle^2
    \]

    \item Define Matrix $A$:
    \[
        A =
        \begin{bmatrix}
            - \mathbf{a}_1^{\mathrm{T}} - \\
            \vdots \\
            - \mathbf{a}_n^{\mathrm{T}} -
        \end{bmatrix}
    \]
    \[
        \sum_{i=1}^{n} \langle \mathbf{a}_i, \mathbf{q}_j \rangle^2 = \|A\mathbf{q}_j\|^2 = \mathbf{q}_{j}^{\mathrm{T}} A^{\mathrm{T}} A \mathbf{q}_j
    \]

    \item New Objective Function:
    \[
        \arg\min_{W} \text{cost}(W) = \arg\max_{W} \sum_{i=1}^{k} \mathbf{q}_i^{\mathrm{T}} A^{\mathrm{T}} A \mathbf{q}_i
    \]

    \item \textbf{Greedy Algorithm:}
    \newline
    Object: \textbf{Conditional Maximization:}
    \[
        \arg\max_{\|\mathbf{q}_i\|=1} \mathbf{q}_i^{\mathrm{T}} A^{\mathrm{T}} A \mathbf{q}_i
    \]
    By \textbf{Lagrange Multiplier Method}:
    \[
        \mathcal{L}(\mathbf{q}_i, \lambda) = \mathbf{q}_i^{\mathrm{T}} A^{\mathrm{T}} A \mathbf{q}_i - \lambda(\mathbf{q}_i^{\mathrm{T}} \mathbf{q}_i - 1)
    \]
    \[
        \frac{\partial \mathcal{L}}{\partial \mathbf{q}_i} = 2 A^{\mathrm{T}} A \mathbf{q}_i - 2\lambda\mathbf{q}_i = \mathbf{0}
    \]
    \[
        A^{\mathrm{T}} A \mathbf{q}_i = \lambda\mathbf{q}_i
    \]
    Solutions:
    \begin{itemize}
        \item $\mathbf{q}_i = \mathbf{v}_i$ is an \textbf{Eigenvector} of $A^{\mathrm{T}} A$
        \item $\lambda = \lambda_i$ is an \textbf{Eigenvalue} of $A^{\mathrm{T}} A$
        \item $\mathbf{q}_i = \mathbf{v}_i$ is a \textbf{Right Singular Vector} of $A$
        \item $\sqrt{\lambda} = \sigma_i$ is a \textbf{Singular Value} of $A$
    \end{itemize}
    \[
        \mathbf{q}_i^{\mathrm{T}} A^{\mathrm{T}} A \mathbf{q}_i = \mathbf{v}_i^{\mathrm{T}} \lambda_i \mathbf{v}_i = \lambda_i = \sigma_i^2
    \]

    \item Therefore:
    \[
        \arg\min_{W} \text{cost}(W) = \arg\max_{W} \sum_{i=1}^{k} \sigma_i^2
    \]
    \[
        W = \text{span}(\{\mathbf{v}_i\}_{i=1}^{k})
    \]
\end{enumerate}

\subsection{PCA}

\textbf{Objective:} Find the $k$-BFS for centered data.
\newline
\textbf{Data matrix} $X \in \mathbb{R}^{n \times d}$:
\[
    X =
    \begin{bmatrix}
        - \mathbf{x}^{(1)T} - \\
        \vdots \\
        - \mathbf{x}^{(n)T} -
    \end{bmatrix}
\]
\textbf{Centered Data Matrix} $B$:
\[
    B = X - \overline{X}
\]
\textbf{Principal Components} $\mathbf{v}_i$ are the \textbf{Right Singular Vectors} of $B$.
\[
    W = \text{span}(\{\mathbf{v}_i\}_{i=1}^{k})
\]
\textbf{Scores} are the data projected onto \textbf{Principal Components}.
\[
    T = BV
\]
\newline
\textbf{Variance:}
\[
    \text{Var}(\mathbf{v}_i^{\mathrm{T}} \mathbf{b}) = \frac{1}{n-1} \|B\mathbf{v}_i\|^2
\]
\[
    \arg \min_W \text{cost}(W) = \arg \max_W \sum_{i=1}^k \text{Var}(\mathbf{v}_i^{\mathrm{T}} \mathbf{b})
\]
\textbf{Sample Covariance Matrix:}
\[
    S = \frac{1}{n-1} B^{\mathrm{T}} B
\]
Let $\lambda_i$ be an \textbf{Eigenvalue} of $S$, $\sigma_i$ be an \textbf{Right Singular Value} of $B$.
\[
    \lambda_i = \frac{\sigma_i^2}{n-1}
\]


\end{document}
