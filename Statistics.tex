\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{amssymb}

\title{Statistics}
\date{}

\begin{document}
\maketitle

\tableofcontents

\newpage
\section{Sample Statistics}

\subsection{Sample Mean}
\[
    \overline{X} = \frac{1}{n}\sum_{i=1}^{n} X_i
\]
\textbf{Properties:}
\begin{itemize}
    \item $\mathbb{E}[\overline{X}] = \mu$
    \item $\text{Var}(\overline{X}) = \frac{\sigma^2}{n}$
\end{itemize}

\subsection{Sample Variance}
\[
    S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \overline{X})^2 = \frac{1}{n-1}\left(\sum_{i=1}^{n} X_i^2 - n\overline{X}^2\right)
\]
\textbf{Properties:}
\begin{itemize}
    \item $\mathbb{E}[S^2] = \sigma^2$
\end{itemize}

\subsection{Sample Covariance}
\[
    Cov(X,Y) = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \overline{X})(Y_i - \overline{Y})
\]
\textbf{Properties:}
\begin{itemize}
    \item $\mathbb{E}[Cov(X,Y)] = \operatorname{cov}(X,Y)$
\end{itemize}

\subsection{Sample Covariance Matrix}
\[
    S = \frac{1}{n-1}\sum_{i=1}^{n}(\mathbf{X}_i - \overline{\mathbf{X}})(\mathbf{X}_i - \overline{\mathbf{X}})^\top
\]
\textbf{Properties:}
\begin{itemize}
    \item $\mathbb{E}[S] = \Sigma$
\end{itemize}

\subsection{Central Limit Theorem}

Let $\{X_i\}_{i=1}^{n}$ be i.i.d. (independent and identically distributed) random variables with mean $\mu$ and variance $\sigma^2$:
\[
    \overline{X}_n \overset{d}{\rightarrow} \mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right) \text{ as } n \rightarrow \infty
\]


\newpage
\section{Sample Distributions}

\subsection{Chi-Square Distribution}
\[
    X \sim \chi^2(n)
\]
where $Z_i \sim \mathcal{N}(0,1)$ and:
\[
    \chi^2 = \sum_{i=1}^{n} Z_i^2
\]
\textbf{Properties:}
\begin{itemize}
    \item $\mathbb{E}[\chi^2] = n$
    \item $\text{Var}(\chi^2) = 2n$
    \item $\chi_1^2 \sim \chi^2(n_1), \chi_2^2 \sim \chi^2(n_2)$: $\chi_1^2 + \chi_2^2 \sim \chi^2(n_1 + n_2)$
    \item $X_i \sim N(\mu, \sigma^2)$: $\chi^2 = \frac{1}{\sigma^2}\sum_{i=1}^{n}(X_i - \mu)^2 \sim \chi^2(n)$
    \item $X_i \sim N(\mu, \sigma^2)$: $\frac{(n-1)S^2}{\sigma^2} = \frac{1}{\sigma^2}\sum_{i=1}^{n}(X_i - \overline{X})^2 \sim \chi^2(n-1)$
\end{itemize}

\subsection{t Distribution}
\[
    t \sim t(n)
\]
where $X \sim \mathcal{N}(0,1)$, $Y \sim \chi^2(n)$, and:
\[
    t = \frac{X}{\sqrt{Y/n}}
\]
\textbf{Properties:}
\begin{itemize}
    \item $n \rightarrow +\infty$: $\mathcal{N}(0,1)$
    \item $T = \frac{\overline{X} - \mu}{S/\sqrt{n}} \sim t(n-1)$
\end{itemize}

\subsection{F Distribution}
\[
    F \sim F(m,n)
\]
where $X \sim \chi^2(m)$, $Y \sim \chi^2(n)$, and:
\[
    F = \frac{X/m}{Y/n}
\]
\textbf{Properties:}
\begin{itemize}
    \item $F_{1-\alpha}(n_1, n_2) = \frac{1}{F_\alpha(n_2, n_1)}$
    \item $t \sim t(n)$: $t^2 \sim F(1, n)$
\end{itemize}


\newpage
\section{Estimation}

\subsection{Maximum Likelihood Estimation (MLE)}
Likelihood Function:
\[
    L_i(\theta) = \mathbb{P}(x|\theta)
\]
\[
    L(\theta|X) = \prod_{i=1}^{n} \mathbb{P}(x_i|\theta)
\]
Log Likelihood:
\[
    \ell(\theta) = \log L(\theta) = \log \mathbb{P}(x|\theta)
\]
\[
    \ell(\theta|X) = \log L(\theta|X) = \sum_{i=1}^{n} \log \mathbb{P}(x_i|\theta)
\]
Maximum Log Likelihood:
\[
    \hat{\theta} = \arg\max_\theta \ell(\theta | X)
\]

\subsubsection{On Bernoulli Distribution}
\[
    y \sim \text{Bernoulli}(\hat{y})
\]
\newline
Likelihood Function:
\[
    L(\theta) = \hat{y}^y (1-\hat{y})^{1-y}
\]
Maximum Log Likelihood:
\[
    \hat{\theta} = \arg\max_{\theta} \ell(\theta) = \arg\max_{\theta} \left[ y \log \hat{y} + (1-y) \log (1-\hat{y}) \right]
\]

\subsubsection{On Categorical Distribution}
\[
    \mathbf{y} \sim \text{Categorical}(\hat{\mathbf{y}})
\]
\newline
Likelihood Function:
\[
    L(\theta) = \prod_{k=1}^{K} \hat{y}_k^{y_k}
\]
Maximum Log Likelihood:
\[
    \hat{\theta} = \arg\max_{\theta} \ell(\theta) = \arg\max_{\theta} \sum_{k=1}^{K} y_k \log \hat{y}_k = \arg\max_{\theta} \mathbf{y}^\top \log \hat{\mathbf{y}}
\]

\subsubsection{On Gaussian Distribution}
\[
    \mathbf{y} \sim \mathcal{N}(\hat{\mathbf{y}}, \sigma^2 I)
\]
\newline
Likelihood Function:
\[
    L(\theta) = \frac{1}{(2\pi\sigma^2)^{D/2}} \exp\left(-\frac{\|\mathbf{y}-\hat{\mathbf{y}}\|^2}{2\sigma^2}\right)
\]
Maximum Log Likelihood:
\[
    \hat{\theta} = \arg\max_{\theta} \ell(\theta) = \arg\min_{\theta} \|\mathbf{y}-\hat{\mathbf{y}}\|^2
\]

\subsubsection{Linear Regression (Ordinary Least Squares)}
For a \textbf{Linear Regression} model:
\[
    A\mathbf{w} = \mathbf{b}
\]
where $\mathbf{b} \notin \text{Col}(A)$ and $\boldsymbol{\epsilon} = \mathbf{b} - A\mathbf{w} \sim \mathcal{N}(\mathbf{0}, \sigma^2 I)$.
\newline
By MLE, the optimal approximate solution (set) is found by minimizing the \textbf{Ordinary Least Squares (OLS)}:
\[
    \min_{\mathbf{w}} \|A\mathbf{w} - \mathbf{b}\|^2
\]

\subsubsection{Ridge Regression (OLS + L2 Regularization)}
\textbf{Ridge Regression} is a regularized version of Linear Regression that uses \textbf{L2 Regularization}.
\newline
In Maximum A Posteriori (MAP) estimation framework, assume prior distribution $\mathbf{w} \sim \mathcal{N}(\mathbf{0}, \tau^2 I)$. By MLE, the optimal solution of OLS is found by:
\[
    \min_{\mathbf{w}} \|A\mathbf{w} - \mathbf{b}\|^2 + \lambda\|\mathbf{{w}}\|_2^2
\]
where the regularization parameter $\displaystyle \lambda = \frac{\sigma^2}{\tau^2}$.

\subsection{Max Entropy}

Sample: $x \in \mathcal{X} = \mathbb{R}^D$, $y \in \mathcal{Y}$
\paragraph{Marginal Distribution:}
\[
    p(y|x) = P(y|x)
\]
\paragraph{Sufficient Statistics:}
\[
    T(y) \in \mathbb{R}^M
\]
\paragraph{Normalization Constraint:}
\[
    \sum_y p(y|x) = 1
\]

\subsubsection{Local Constraint}

\paragraph{Principal of Maximum Entropy:}
\[
    H(Y|X=x) = - \sum_y p(y|x) \ln p(y|x)
\]
\paragraph{Moment Matching:} \mbox{}
\newline
Local Constraint:
\[
    \mathbb{E}[T(Y)|x] = \hat{\mathbb{E}}[T(Y)|x] \in \mathbb{R}^M
\]
\[
    \sum_y p(y|x) T(y) = \hat{\mathbb{E}}[T(Y)|x]
\]
\paragraph{Lagrange Multiplier:}
\begin{align*}
    \mathcal{L}(p, \alpha, \lambda) &= - \sum_y p(y|x) \ln p(y|x) + \alpha(x) \left(\sum_y p(y|x) - 1\right) \\
    &\quad + \left\langle \lambda(x), \sum_y p(y|x) T(y) - \hat{\mathbb{E}}[T(Y)|x] \right\rangle
\end{align*}
where $\lambda \in \mathbb{R}^M$.
\[
    \frac{\partial \mathcal{L}}{\partial p(y|x)} = - (1 + \ln p(y|x)) + \alpha(x) + \lambda(x)^{\mathrm{T}} T(y) = 0
\]
\[
    p(y|x) = \exp(\alpha(x) - 1) \cdot \exp(\lambda^{\mathrm{T}} T(y))
\]
\[
    p(y|x) = \frac{\exp(\lambda(x)^{\mathrm{T}} T(y))}{\sum_{y \in \mathcal{Y}} \exp(\lambda(x)^{\mathrm{T}} T(y))}
\]
where $\displaystyle \frac{\sum T(y) \exp(\lambda^{\mathrm{T}} T(y))}{\sum_{y \in \mathcal{Y}} \exp(\lambda^{\mathrm{T}} T(y))} = \hat{\mathbb{E}}[T(Y)|x]$.

\subsubsection{Global Constraint}

\paragraph{Principal of Maximum Entropy:}
\[
    H(Y|X) = \sum_x \tilde{p}(x) H(Y|X=x) = - \sum_x \tilde{p}(x) \sum_y p(y|x) \ln p(y|x)
\]
\paragraph{Moment Matching:} \mbox{}
\newline
Global Constraint:
\[
    \mathbb{E}[T(Y)x^{\mathrm{T}}] = \hat{\mathbb{E}}[T(Y)x^{\mathrm{T}}] \in \mathbb{R}^{M \times D}
\]
\[
    \sum_x \tilde{p}(x) \sum_y p(y|x) T(y)x^{\mathrm{T}} = \hat{\mathbb{E}}[T(Y)x^{\mathrm{T}}]
\]
Marginal Constraint:
\[
    \mathbb{E}[T(Y)] = \hat{\mathbb{E}}[T(Y)] \in \mathbb{R}^M
\]
\[
    \sum_x \tilde{p}(x) \sum_y p(y|x) T(y) = \hat{\mathbb{E}}[T(Y)]
\]
\paragraph{Lagrange Multiplier:}
\begin{align*}
    \mathcal{L}(p, \alpha, W, b) &= - \sum_x \tilde{p}(x) \sum_y p(y|x) \ln p(y|x) + \sum_x \alpha(x) \left(\sum_y p(y|x) - 1\right) \\
    &\quad + \left\langle W, \sum_x \tilde{p}(x) \sum_y p(y|x) T(y)x^{\mathrm{T}} - \hat{\mathbb{E}}[T(Y)x^{\mathrm{T}}] \right\rangle \\
    &\quad + \left\langle b, \sum_x \tilde{p}(x) \sum_y p(y|x) T(y) - \hat{\mathbb{E}}[T(Y)] \right\rangle
\end{align*}
where $W \in \mathbb{R}^{M \times D}$, $b \in \mathbb{R}^M$.
\[
    \frac{\partial \mathcal{L}}{\partial p(y|x)} = - \tilde{p}(x) (1 + \ln p(y|x)) + \alpha(x) + \tilde{p}(x) \langle W, T(y)x^{\mathrm{T}} \rangle + \tilde{p}(x) \langle b, T(y) \rangle
\]
\[
    p(y|x) = \exp\left(\frac{\alpha(x)}{\tilde{p}(x)} - 1\right) \cdot \exp((Wx+b)^{\mathrm{T}} T(y))
\]
Let $z = Wx+b$,
\[
    p(y|x) = \frac{\exp(z^{\mathrm{T}} T(y))}{\sum_{y \in \mathcal{Y}} \exp(z^{\mathrm{T}} T(y))}
\]
where $\displaystyle \frac{\sum_x \tilde{p}(x) \sum_y \exp(z^{\mathrm{T}} T(y)) T(y)x^{\mathrm{T}}}{\sum_{y \in \mathcal{Y}} \exp(z^{\mathrm{T}} T(y))} = \hat{\mathbb{E}}[T(Y)x^{\mathrm{T}}]$, $\displaystyle \frac{\sum_x \tilde{p}(x) \sum_y \exp(z^{\mathrm{T}} T(y)) T(y)}{\sum_{y \in \mathcal{Y}} \exp(z^{\mathrm{T}} T(y))} = \hat{\mathbb{E}}[T(Y)]$.

\subsubsection{On Bernoulli Distribution}
\[
    y \sim \text{Bernoulli}(p)
\]
$T(y) = y \in \mathbb{R}$, $W \in \mathbb{R}^{1 \times D}$
\[
    p(Y=1|x) = \frac{1}{1 + \exp(-z)}
\]

\subsubsection{On Categorical Distribution}
\[
    \mathbf{y} \sim \text{Categorical}(\mathbf{p})
\]
$T(\mathbf{y}) = \mathbf{y} \in \mathbb{R}^K$, $W \in \mathbb{R}^{K \times D}$
\[
    p(Y=\mathbf{y}|x) = \frac{\exp(\mathbf{z}^{\mathrm{T}} \mathbf{y})}{\sum_{k=1}^K \exp(z_k)}
\]

\subsubsection{On Gaussian Distribution}
\[
    y \sim \mathcal{N}(\mu, \sigma^2)
\]
$T(y) = \begin{bmatrix} y \\ y^2 \end{bmatrix} \in \mathbb{R}^2$, $W \in \mathbb{R}^{2 \times D}$
\[
    p(Y=y|x) = \frac{\exp(z_1 y + z_2 y^2)}{\int_{-\infty}^{\infty} \exp(z_1 y' + z_2 (y')^2) dy'} = \sqrt{\frac{-z_2}{\pi}} \exp\left(z_2\left(y + \frac{z_1}{2z_2}\right)^2\right) \quad (z_2 < 0 \text{ for convergence})
\]

\subsection{Interval Estimation}

For significance level $\alpha$:
\[
    \mathbb{P}(\underline{\theta} < \theta < \overline{\theta}) \geq 1 - \alpha
\]

\subsubsection{Single Population}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Unknown} & \textbf{Known} & \textbf{Distribution} & \textbf{Two-tailed Confidence Interval} \\
\hline
$\mu$ & $\sigma^2$ & $u = \frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \sim \mathcal{N}(0,1)$ & $\left(\overline{X} \pm \frac{\sigma}{\sqrt{n}} u_{\alpha/2}\right)$ \\
\hline
$\mu$ & $S^2$ & $t = \frac{\overline{X} - \mu}{S/\sqrt{n}} \sim t(n-1)$ & $\left(\overline{X} \pm \frac{S}{\sqrt{n}} t_{\alpha/2}(n-1)\right)$ \\
\hline
$\sigma^2$ & $S^2$ & $\chi^2 = \frac{(n-1)S^2}{\sigma^2} \sim \chi^2(n-1)$ & $\left(\frac{(n-1)S^2}{\chi^2_{\alpha/2}(n-1)}, \frac{(n-1)S^2}{\chi^2_{1-\alpha/2}(n-1)}\right)$ \\
\hline
\end{tabular}
\end{table}

\subsubsection{Two Populations}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Unknown} & \textbf{Known} & \textbf{Distribution} & \textbf{Two-tailed Confidence Interval} \\
\hline
$\mu_1 - \mu_2$ & $\sigma_1^2, \sigma_2^2$ & $\frac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}} \sim \mathcal{N}(0,1)$ & $\left((\overline{X} - \overline{Y}) \pm u_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}\right)$ \\
\hline
$\mu_1 - \mu_2$ & $S_1^2, S_2^2$ & $\frac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}{S_W\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim t(n_1 + n_2 - 2)$ & $\left((\overline{X} - \overline{Y}) \pm t_{\alpha/2}(n_1 + n_2 - 2) S_W\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}\right)$ \\
& & $S_W^2 = \frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1 + n_2 - 2}$ & \\
\hline
$\frac{\sigma_1^2}{\sigma_2^2}$ & $S_1^2, S_2^2$ & $\frac{S_1^2/S_2^2}{\sigma_1^2/\sigma_2^2} \sim F(n_1-1, n_2-1)$ & $\left(\frac{S_1^2/S_2^2}{F_{1-\alpha/2}(n_1-1, n_2-1)}, \frac{S_1^2/S_2^2}{F_{\alpha/2}(n_1-1, n_2-1)}\right)$ \\
\hline
\end{tabular}
\end{table}

\section{Principal Component Analysis (PCA)}

\subsection{Best Fitting Subspace (BFS)}

\begin{enumerate}
    \item \textbf{Objective:}
    \[
        \arg\min_{W} \text{cost}(W)
    \]
    where \textbf{Cost Function:}
    \[
        \text{cost}(W) = \sum_{i=1}^{n} \|\mathbf{a}_i - P_W \mathbf{a}_i\|^2
    \]

    \item By the \textbf{Pythagorean Theorem:}
    \[
        \|\mathbf{a}_i - P_W \mathbf{a}_i\|^2 = \|\mathbf{a}_i\|^2 - \|P_W \mathbf{a}_i\|^2
    \]
    \[
        \sum_{i=1}^{n} \|\mathbf{a}_i - P_W \mathbf{a}_i\|^2 = \sum_{i=1}^{n} \|\mathbf{a}_i\|^2 - \sum_{i=1}^{n} \|P_W \mathbf{a}_i\|^2
    \]

    \item Let $\{\mathbf{q}_j\}_{j=1}^{k}$ be an \textbf{orthonormal basis} for $W$.
    By \textbf{Parseval's Identity:}
    \[
        \|P_W \mathbf{a}_i\|^2 = \sum_{j=1}^{k} \langle \mathbf{a}_i, \mathbf{q}_j \rangle^2
    \]
    \[
        \sum_{i=1}^{n} \|P_W \mathbf{a}_i\|^2
        = \sum_{i=1}^{n} \sum_{j=1}^{k} \langle \mathbf{a}_i, \mathbf{q}_j \rangle^2
        = \sum_{j=1}^{k} \sum_{i=1}^{n} \langle \mathbf{a}_i, \mathbf{q}_j \rangle^2
    \]

    \item Define Matrix $A$:
    \[
        A =
        \begin{bmatrix}
            - \mathbf{a}_1^{\mathrm{T}} - \\
            \vdots \\
            - \mathbf{a}_n^{\mathrm{T}} -
        \end{bmatrix}
    \]
    \[
        \sum_{i=1}^{n} \langle \mathbf{a}_i, \mathbf{q}_j \rangle^2 = \|A\mathbf{q}_j\|^2 = \mathbf{q}_{j}^{\mathrm{T}} A^{\mathrm{T}} A \mathbf{q}_j
    \]

    \item New Objective Function:
    \[
        \arg\min_{W} \text{cost}(W) = \arg\max_{W} \sum_{i=1}^{k} \mathbf{q}_i^{\mathrm{T}} A^{\mathrm{T}} A \mathbf{q}_i
    \]

    \item \textbf{Greedy Algorithm:}
    \newline
    Object: \textbf{Conditional Maximization:}
    \[
        \arg\max_{\|\mathbf{q}_i\|=1} \mathbf{q}_i^{\mathrm{T}} A^{\mathrm{T}} A \mathbf{q}_i
    \]
    By \textbf{Lagrange Multiplier Method}:
    \[
        \mathcal{L}(\mathbf{q}_i, \lambda) = \mathbf{q}_i^{\mathrm{T}} A^{\mathrm{T}} A \mathbf{q}_i - \lambda(\mathbf{q}_i^{\mathrm{T}} \mathbf{q}_i - 1)
    \]
    \[
        \frac{\partial \mathcal{L}}{\partial \mathbf{q}_i} = 2 A^{\mathrm{T}} A \mathbf{q}_i - 2\lambda\mathbf{q}_i = \mathbf{0}
    \]
    \[
        A^{\mathrm{T}} A \mathbf{q}_i = \lambda\mathbf{q}_i
    \]
    Solutions:
    \begin{itemize}
        \item $\mathbf{q}_i = \mathbf{v}_i$ is an \textbf{Eigenvector} of $A^{\mathrm{T}} A$
        \item $\lambda = \lambda_i$ is an \textbf{Eigenvalue} of $A^{\mathrm{T}} A$
        \item $\mathbf{q}_i = \mathbf{v}_i$ is a \textbf{Right Singular Vector} of $A$
        \item $\sqrt{\lambda} = \sigma_i$ is a \textbf{Singular Value} of $A$
    \end{itemize}
    \[
        \mathbf{q}_i^{\mathrm{T}} A^{\mathrm{T}} A \mathbf{q}_i = \mathbf{v}_i^{\mathrm{T}} \lambda_i \mathbf{v}_i = \lambda_i = \sigma_i^2
    \]

    \item Therefore:
    \[
        \arg\min_{W} \text{cost}(W) = \arg\max_{W} \sum_{i=1}^{k} \sigma_i^2
    \]
    \[
        W = \text{span}(\{\mathbf{v}_i\}_{i=1}^{k})
    \]
\end{enumerate}

\subsection{PCA}

\textbf{Objective:} Find the $k$-BFS for centered data.
\newline
\textbf{Data matrix} $X \in \mathbb{R}^{n \times d}$:
\[
    X =
    \begin{bmatrix}
        - \mathbf{x}^{(1)T} - \\
        \vdots \\
        - \mathbf{x}^{(n)T} -
    \end{bmatrix}
\]
\textbf{Centered Data Matrix} $B$:
\[
    B = X - \overline{X}
\]
\textbf{Principal Components} $\mathbf{v}_i$ are the \textbf{Right Singular Vectors} of $B$.
\[
    W = \text{span}(\{\mathbf{v}_i\}_{i=1}^{k})
\]
\textbf{Scores} are the data projected onto \textbf{Principal Components}.
\[
    T = BV
\]
\newline
\textbf{Variance:}
\[
    \text{Var}(\mathbf{v}_i^{\mathrm{T}} \mathbf{b}) = \frac{1}{n-1} \|B\mathbf{v}_i\|^2
\]
\[
    \arg \min_W \text{cost}(W) = \arg \max_W \sum_{i=1}^k \text{Var}(\mathbf{v}_i^{\mathrm{T}} \mathbf{b})
\]
\textbf{Sample Covariance Matrix:}
\[
    S = \frac{1}{n-1} B^{\mathrm{T}} B
\]
Let $\lambda_i$ be an \textbf{Eigenvalue} of $S$, $\sigma_i$ be an \textbf{Right Singular Value} of $B$.
\[
    \lambda_i = \frac{\sigma_i^2}{n-1}
\]


\end{document}
