# Machine Learning Tutorials

## Overview

- 3Blue1Brown: Neural networks
- Welch Labs: Neural Networks Demystified

## General

- Modeling
  - The Exponential Family
    - Mutual Information: The Exponential Family
- Optimization
  - Backpropagation
    - Artem Kirsanov: The Most Important Algorithm in Machine Learning
    - DeepBean: Backpropagation: How Neural Networks Learn
  - Gradient Methods
    - DeepBean: Optimization for Deep Learning (Momentum, RMSprop, AdaGrad, Adam)
  - Normalization
    - Assembly AI
      - Batch normalization | What it is and how to implement it
      - What is Layer Normalization? | Deep Learning Fundamentals
- Boosting
  - StatQuest with Josh Starmer: AdaBoost, Clearly Explained
- Explainable AI
  - DeepFindr: Explainable AI

## Methods

- Variational Inference
  - Artem Kirsanov: How Neural Networks Handle Probabilities
  - DeepBean
    - Understanding Variational Autoencoders (VAEs)
    - Vector-Quantized Variational Autoencoders (VQ-VAEs)
    - Disentanglement with beta-VAEs
- Boltzmann Machine
  - Artem Kirsanov
    - A Brain-Inspired Algorithm For Memory
    - Generative Model That Won 2024 Nobel Prize
- Graph Theory
  - DeepFindr: Graph Neural Networks
  - Ron & Math: Spectral Graph Theory For Dummies
- CNN
  - DeepBean
    - How CNNs Work (Convolutional Neural Nets)
    - How YOLO Object Detection Works
- RNN
  - DeepBean: Vanishing Gradients: Why Training RNNs is Hard
- Transformer
  - DeepBean: Transformers, Simply Explained | Deep Learning
  - Efficient NLP: The KV Cache: Memory Usage in Transformers
  - Julia Turc: Why are Transformers replacing CNNs?
- GAN
  - DeepBean: Understanding GANs (Generative Adversarial Networks)
- Diffusion
  - DeepBean: Diffusion Models
- MiniMax
  - Sebastian Lague: Algorithms Explained â€“ minimax and alpha-beta pruning
